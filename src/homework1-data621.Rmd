---
title: 'DATA 621: Homework 1 (Baseball Regression)'
author: "Shoshana Farber, Josh Forster, Glen Davis, Andrew Bowen, Charles Ugiagbe"
date: "October 1, 2023"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setup:

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(glue)
library(tidyverse)
library(car)
library(ResourceSelection)
```

First, let's read in the provided dataset.

```{r data-read-in, echo=FALSE, message=FALSE}
df <- read.csv("https://raw.githubusercontent.com/andrewbowen19/businessAnalyticsDataMiningDATA621/main/data/moneyball-training-data.csv")
df <- data.frame(df)
```

## Data Exploration:

```{r dimensions, echo=F}
dim = dim(df)

print(glue("The dataset consists of {dim[1]} observations of {dim[2]} variables."))
```

The variables and their definitions can be seen below:

Variable|Definition
---|---
`INDEX`|Identification variable
`TARGET_WINS`|Number of wins
`TEAM_BATTING_H`|Base hits by batters (1B, 2B, 3B, HR)
`TEAM_BATTING_2B`|Doubles by batters (2B)
`TEAM_BATTING_3B`|Triples by batters (3B)
`TEAM_BATTING_HR`|Homeruns by batters (4B)
`TEAM_BATTING_BB`|Walks by batters
`TEAM_BATTING_HBP`|Batters hit by pitch (get a free base)
`TEAM_BATTING_SO`|Strikeouts by batters
`TEAM_BASERUN_SB`|Stolen bases
`TEAM_BASRUN_CS`|Caught stealing
`TEAM_FIELDING_E`|Errors
`TEAM_FIELDING_DP`|Double plays
`TEAM_PITCHING_BB`|Walks allowed
`TEAM_PITCHING_H`|Hits allowed
`TEAM_PITCHING_HR`|Homeruns allowed
`TEAM_PITCHING_SO`|Strikeouts by pitchers

`INDEX` is an identifying feature and should not be included in the linear regression model. 

```{r train-data, echo=FALSE}
train <- subset(df, select=-c(INDEX))
```

Next, let's print out some summary statistics. We're primarily interested in the `TARGET_WINS` variable, so we'll look at that first.

```{r echo=FALSE, message=FALSE}
mean_wins <- mean(train$TARGET_WINS)
median_wins <- median(train$TARGET_WINS)
sd_wins <- sd(train$TARGET_WINS)

# Print summary stats
print(glue("The mean number of wins in a season is {round(mean_wins,2)}."))
print(glue("The median number of wins in a season is {median_wins}."))
print(glue("The standard deviation for number of wins in a season is {round(sd_wins,2)}."))
```

Let's also make a histogram of the `TARGET_WINS` variable. This should give us a sense of the distribution of wins for teams/seasons in our population.

```{r wins-hist, echo=FALSE, message=FALSE, fig.show='hold', fig.align='center', out.width='85%'}
ggplot(train, aes(x=TARGET_WINS)) + 
  geom_histogram() +
  labs(title = "Distribution of Wins (Histogram)", x = "Number of Wins", y = "Count")
```

Overall, the number of wins in a season for a given baseball team looks fairly normally distributed. Looking at a boxplot helps to highlight the outliers.

```{r box-plot, echo=FALSE, fig.show='hold', fig.align='center', out.width='85%'}
ggplot(train, aes(x=TARGET_WINS)) + 
  geom_boxplot(fill="darkgrey") +
  labs(title = "Distribution of Wins (Boxplot)", x = "Number of Wins", y = "Count")
```

Let's take a look at the summary statistics for all the variables:

```{r summary-stats, echo=F}
summary(train)
```

We can see quite a few NA values for `TEAM_BATTING_SO`, `TEAM_BASERUN_SB`, `TEAM_BASERUN_CS`, `TEAM_BATTING_HBP`, `TEAM_PITCHING_SO`, and `TEAM_FIELDING_DP`. Let's take a look at the distributions of these variables to see how to impute these missing values.  

```{r dist-na-vars, echo=FALSE}
par(mfrow=c(2,3))
par(mai=c(.3,.3,.3,.3))

variables <- c("TEAM_BATTING_SO", "TEAM_BASERUN_SB", "TEAM_BASERUN_CS", "TEAM_BATTING_HBP", "TEAM_PITCHING_SO", "TEAM_FIELDING_DP")

for (i in 1:(length(variables))) {
  hist(train[[variables[i]]], main = variables[i], col = "lightblue")
}
```

`TEAM_BASERUN_SB`, `TEAM_BASERUN_CS` and `TEAM_PITCHING_SO` seem to be skewed to the right so we should probably impute the missing values using the median value for these variables. `TEAM_BATTING_HBP` and `TEAM_FIELDING_DP` seem basically normally distributed so we can use the mean here, although `TEAM_BATTING_HBP` has 2,085 NA values out of 2,276 observations so it may make sense to leave this variable out of our model entirely. `TEAM_BATTING_SO` is bimodally distributed. 

```{r impute-vars, echo=FALSE}
train_imputed <- train |>
  mutate(TEAM_BASERUN_SB = replace(TEAM_BASERUN_SB, is.na(TEAM_BASERUN_SB), median(TEAM_BASERUN_SB, na.rm=T)),
         TEAM_BASERUN_CS = replace(TEAM_BASERUN_CS, is.na(TEAM_BASERUN_CS), median(TEAM_BASERUN_CS, na.rm=T)),
         TEAM_PITCHING_SO = replace(TEAM_PITCHING_SO, is.na(TEAM_PITCHING_SO), median(TEAM_PITCHING_SO, na.rm=T)),
         TEAM_BATTING_HBP = replace(TEAM_BATTING_HBP, is.na(TEAM_BATTING_HBP), mean(TEAM_BATTING_HBP, na.rm=T)),
         TEAM_FIELDING_DP = replace(TEAM_FIELDING_DP, is.na(TEAM_FIELDING_DP), mean(TEAM_FIELDING_DP, na.rm=T)))
```

Let's look at raw correlations between our other included variables and a team's win total for a season: 

```{r basic-correlations, echo=FALSE, message=FALSE}
cor(train_imputed, df$TARGET_WINS)
```
Let's review relationships between batting independent variables

```{r batting_rel,echo=FALSE,message=FALSE}
subset_batting <- na.omit(train_cleaned |> select(contains('Batting')))
ResourceSelection::kdepairs(subset_batting)
```
Most of the batting variables appear to be somewhat approximately normal although there are some cases of right skew. Overall, there aren't any very strong correlations between these statistics at least from a preliminary visual inspection.

Let's review relationships between other independent variables

```{r pitching_rel,echo=FALSE,message=FALSE}
subset_pitching <- na.omit(train_cleaned |> select(!contains('batting')))
ResourceSelection::kdepairs(subset_pitching)
```
There isn't very strong correlation between the other independent variables similar to the batting statistics although there are more examples of skewed data with these inputs.

Let's make a basic model with all inputs:

```{r scatter-plots, echo=FALSE, message=FALSE}
lm_all <- lm(TARGET_WINS~., train)
summary(lm_all)
```

We can see that the $R^2$ value of a model that includes all the variables is not particularly high. Given the fact that the F-score has a very low p-value and there aren't many significant independent variables it is fairly obvious that there are potential multicollinearity issues within this model.


Reviewing the variance inflation factors

```{r}
vif(lm_all)
```
This model is suffering from considerable impact from the number of collinear predictors as multiple columns are off the charts given that a score of 5 tends to be far too much collinearity in a least squares regression model.

Let's review the dependent and collinear independent variables

```{r collinear_cols}
collinear_cols <- na.omit(train_cleaned |> select(TARGET_WINS,BATTING_H,BATTING_HR,BATTING_BB,BATTING_SO,PITCHING_H,PITCHING_HR,PITCHING_BB,PITCHING_SO))
kdepairs(collinear_cols)

```
Interestingly enough the correlation between these predictors from a preliminary visual review does not seem to identify a substantial amount of collinearity outside of `TEAM_PITCHED_HR` and `TEAM_BATTING_HR`. A few of the other variables potentially have non-linear relationships between them although `TEAM_PITCHING_SO` appears to have several large outliers that might be impacting the model.

We can make some plots to help test our assumptions of our basic model using the `plot` function on our model variable:

```{r echo=FALSE, message=FALSE}
plot(lm_all)
```

Removing the variables that are least significant to the all-variable model one by one via backward selection, we can produce a reduced, better model:

```{r backward_selection, echo=FALSE, message=FALSE, warning=FALSE}
# This model will need to be run again once we deal with imputing the missing values for certain variables. 
lm_reduced <- update(lm_all, . ~ . - TEAM_BATTING_SO)
lm_reduced <- update(lm_reduced, . ~ . - TEAM_BASERUN_CS)
lm_reduced <- update(lm_reduced, . ~ . - TEAM_BATTING_HR)
lm_reduced <- update(lm_reduced, . ~ . - TEAM_BATTING_2B)
lm_reduced <- update(lm_reduced, . ~ . - TEAM_PITCHING_H)
lm_reduced <- update(lm_reduced, . ~ . - TEAM_BATTING_BB)
lm_reduced <- update(lm_reduced, . ~ . - TEAM_BASERUN_SB)
lm_reduced <- update(lm_reduced, . ~ . - TEAM_BATTING_3B)
summary(lm_reduced)
```
Let's review VIF in the reduced model

```{r reduced_lm_vif, echo=FALSE,message=FALSE}
vif(lm_reduced)
```
Taking out a number of the independent variables has substantially reduced collinearity within this revised linear model and it is not an issue in this instance of the model.


Now we can make a model with inputs that we know from baseball.

- Total hits (`TEAM_BATTING_H`)
- Total walks gained (`TEAM_BATTING_BB`)
- Total hits allowed (`TEAM_PITCHING_H`)
- Total walks allowed (`TEAM_PITCHING_BB`)

We chose these variables based on our understanding that good teams generally tend to get on base more frequently (positive predictor variables `TEAM_BATTING_HITS` and `TEAM_BATTING_BB`) while allowing *fewer* runners on base (negative predictor variables `TEAM_PITCHING_H` and `TEAM_PITCHING_BB`).

```{r lm-select, echo=FALSE, message=FALSE}
# Create model with select inputs (walks and hits allowed/gained)
lm_select <- lm(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_PITCHING_H + TEAM_PITCHING_BB, train)

summary(lm_select)
```

```{r plot-lm-select, echo=FALSE, message=FALSE}
plot(lm_select)
```

It's interesting to note that with selected variables (walks and hits gained/allowed per team) that our adjusted $R^2$ actually went *down*, indicating the amount of variability in `TARGET_WINS` explained by our more selective walks/hits model is *less* than the model including all variables.

Looking at our residual plot above, there seems to be a clustering of residuals along the x-axis at $X \approx 80$. This shows a pattern in our residuals.

```{r plot-lm-select-residuals, echo=FALSE, message=FALSE}

# Plot selective model residuals
plot(lm_select$residuals)

```

Let's plot our response variable (*Total Wins*) versus each of our predictor variables to get a sense of linear relationships.

```{r plot-select-vars, echo=FALSE, message=FALSE}

# Plot our response variable for each predictor variable to get a sense of
plot(TARGET_WINS ~ TEAM_BATTING_H + TEAM_BATTING_BB + TEAM_PITCHING_H + TEAM_PITCHING_BB, data=train)

```

## Model Evaluation: 

We'll need to read in our [evaluation data](https://github.com/andrewbowen19/businessAnalyticsDataMiningDATA621/blob/main/data/moneyball-evaluation-data.csv), which is hosted on GitHub for reproducability. 

```{r echo=FALSE}
eval_data_url <- "https://raw.githubusercontent.com/andrewbowen19/businessAnalyticsDataMiningDATA621/main/data/moneyball-evaluation-data.csv"

test <- read.csv(eval_data_url)
```

```{r}
predict(lm_all, test)
```

# Appendix: Report Code

Below is the code for this report to generate the models and charts above.

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```

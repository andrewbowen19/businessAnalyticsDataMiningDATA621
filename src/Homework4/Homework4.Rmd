---
title: "DATA 621 - HW4"
author: "Andrew Bowen, Glen Davis, Shoshana Farber, Joshua Forster, Charles Ugiagbe"
date: "2023-10-30"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Homework 4 - Binary Logistic Regression & Multiple Linear Regression

```{r packages, warning=FALSE, message = FALSE}
library(tidyverse)
library(DataExplorer)
library(knitr)
library(mice)
library(cowplot)

```

```{r theme}
cur_theme <- theme_set(theme_classic())

```

### Data Exploration:

We load an auto insurance company dataset containing 8,161 records. Each record represents a customer, and each record has two response variables: `TARGET_FLAG` and `TARGET_AMT`. Below is a short description of all the variables of interest in the data set, including these response variables:

```{r data1}
my_url <- "https://raw.githubusercontent.com/andrewbowen19/businessAnalyticsDataMiningDATA621/main/data/insurance_training_data.csv"
main_df <- read.csv(my_url, na.strings = "")

```

|VARIABLE NAME|DEFINITION|
|--|--|
|`INDEX`|Identification Variable|
|`TARGET_FLAG`|Was Car in a crash? 1=YES 0=NO|
|`TARGET_AMT`|If car was in a crash, what was the cost|
|`AGE`|Age of Driver|
|`BLUEBOOK`|Value of Vehicle|
|`CAR_AGE`|Vehicle Age|
|`CAR_TYPE`|Type of Car|
|`CAR_USE`|Vehicle Use|
|`CLM_FREQ`|# Claims (Past 5 Years)|
|`EDUCATION`|Max Education Level|
|`HOMEKIDS`|# Children at Home|
|`HOME_VAL`|Home Value|
|`INCOME`|Income|
|`JOB`|Job Category|
|`KIDSDRIV`|# Driving Children|
|`MSTATUS`|Marital Status|
|`MVR_PTS`|Motor Vehicle Record Points|
|`OLDCLAIM`|Total Claims (Past 5 Years)|
|`PARENT1`|Single Parent|
|`RED_CAR`|A Red Car|
|`REVOKED`|License Revoked (Past 7 Years)|
|`SEX`|Gender|
|`TIF`|Time in Force|
|`TRAVTIME`|Distance to Work|
|`URBANICITY`|Home/Work Area|
|`YOJ`|Years on Job|

We take a look at the classes of our variables. 

```{r data_classes}
classes <- as.data.frame(unlist(lapply(main_df, class))) |>
    rownames_to_column()
cols <- c("Variable", "Class")
colnames(classes) <- cols
classes_summary <- classes |>
    group_by(Class) |>
    summarize(Count = n(),
              Variables = paste(sort(unique(Variable)),collapse=", "))
knitr::kable(classes_summary, format = "simple")

```

`INCOME`, `HOME_VAL`, `BLUEBOOK`, and `OLDCLAIM` are all character columns that need to be recoded as integers. `TARGET_FLAG` and the remaining character columns will all need to be recoded as factors. 

```{r data_char_int_recode}
vars <- c("INCOME", "HOME_VAL", "BLUEBOOK", "OLDCLAIM")
main_df <- main_df |>
    mutate(across(all_of(vars), ~gsub("\\$|,", "", .) |> as.integer()))

```

We remove the identification variable `INDEX` and take a look at a summary of the dataset's completeness.

```{r data2}
main_df <- main_df |>
    select(-INDEX)
remove <- c("discrete_columns", "continuous_columns",
            "total_observations", "memory_usage")
completeness <- introduce(main_df) |>
    select(-all_of(remove))
knitr::kable(t(completeness), format = "simple")

```

None of our columns are completely devoid of data. There are 6,045 complete rows in the dataset, which is about 74% of our observations. There are 2,405 total missing values. We take a look at which variables contain these missing values and what the spread is.

```{r data3, include = FALSE}
p1 <- plot_missing(main_df, missing_only = TRUE,
                   ggtheme = theme_classic(), title = "Missing Values")

```

```{r data4, warning = FALSE, message = FALSE}
p1 <- p1 + 
    scale_fill_brewer(palette = "Paired")
p1

```

A very small percentage of observations contain missing `AGE` values. The `INCOME`, `YOJ`, `HOME_VAL`, `CAR_AGE`, and `JOB` variables are each missing around 5.5 to 6.5 percent of values. There are no variables containing such extreme proportions of missing values that removal would be warranted on that basis alone.

We have 14 numeric variables and 11 categorical variables (including the dummy variable `TARGET_FLAG`). We recode the categorical variables as factors and list the possible ranges or values for each variable in the breakdown below:

```{r data_cont_vs_disc}
output <- split_columns(main_df, binary_as_factor = TRUE)
num <- data.frame(Variable = names(output$continuous),
                   Type = rep("Numeric", ncol(output$continuous)))
cat <- data.frame(Variable = names(output$discrete),
                   Type = rep("Categorical", ncol(output$discrete)))
ranges <- as.data.frame(t(sapply(main_df |> select(-names(output$discrete)),
                                 range, na.rm = TRUE)))
factors <- names(output$discrete)
main_df <- main_df |> 
    mutate(across(all_of(factors), ~as.factor(.)))
values <- as.data.frame(t(sapply(main_df |> select(all_of(factors)),
                                 levels)))
values <- values |>
    mutate(across(all_of(factors), ~toString(unlist(.))))
values <- as.data.frame(t(values)) |>
    rownames_to_column()
cols <- c("Variable", "Values")
colnames(values) <- cols
remove <- c("V1", "V2")
ranges <- ranges |>
    rownames_to_column() |>
    group_by(rowname) |>
    mutate(Values = toString(c(V1, " - ", round(V2, 1))),
           Values = str_replace_all(Values, ",", "")) |>
    select(-all_of(remove))
colnames(ranges) <- cols
num <- num |>
    merge(ranges)
cat <- cat |>
    merge(values)
num_vs_cat <- num |>
    bind_rows(cat)
knitr::kable(num_vs_cat, format = "simple")

```

Some of the factor levels are named and leveled inconsistently, so we will rename and relevel them in the next section.

Let's take a look at the summary statistics for each variable.

```{r desc-stats, echo=F}
summary(main_df)

```

There are 6 NAs in `AGE`, 454 in `YOJ`, and 510 in `CAR_AGE`. 

Let's take a look at the distributions of the numeric variables.

```{r}
# just numeric variables
numeric_train <- main_df[,sapply(main_df, is.numeric)]
par(mfrow=c(4,4))
par(mai=c(.3,.3,.3,.3))
variables <- names(numeric_train)
for (i in 1:(length(variables))) {
  hist(numeric_train[[variables[i]]], main = variables[i], col = "lightblue")
}

```

Let's also take a look at the distributions of the categorical variables. First, we look at the distributions for categorical variables with only two levels. 

```{r cat_dist, warning = FALSE, message = FALSE}
cat_pivot <- main_df |>
    select(all_of(factors)) |>
    pivot_longer(cols = all_of(factors),
                 names_to = "Variable",
                 values_to = "Value") |>
    group_by(Variable, Value) |>
    summarize(Count = n()) |>
    group_by(Variable) |>
    mutate(Levels = n()) |>
    ungroup()
p2 <- cat_pivot |>
    filter(Levels == 2) |>
    ggplot(aes(x = Value, y = Count)) +
    geom_col(fill = "lightblue", color = "black") +
    facet_wrap(vars(Variable), ncol = 4, scales = "free_x") +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
p2

```

Next we look at the distributions for the categorical variables with more than two levels.

```{r }
p3 <- cat_pivot |>
    filter(Levels > 2) |>
    ggplot(aes(x = Value, y = Count)) +
    geom_col(fill = "lightblue", color = "black") +
    coord_flip() + 
    facet_wrap(vars(Variable), ncol = 1, scales = "free")
p3

```

### Data Preparation

First, we rename and relevel the inconsistently named and leveled factor variables we noted earlier.

```{r rename_relevel_factors}
x <- main_df$CAR_TYPE
main_df$CAR_TYPE <- case_match(x, "z_SUV" ~ "SUV", .default = x)
main_df$CAR_TYPE <- factor(main_df$CAR_TYPE,
                           levels = c("SUV", "Minivan", "Panel Truck",
                                      "Pickup", "Sports Car", "Van"))
x <- main_df$EDUCATION
main_df$EDUCATION <- case_match(x, "z_High School" ~ "High School", .default = x)
main_df$EDUCATION <- factor(main_df$EDUCATION,
                             levels = c("<High School", "High School",
                                        "Bachelors", "Masters", "PhD"))
x <- main_df$JOB
main_df$JOB <- case_match(x, "z_Blue Collar" ~ "Blue Collar", .default = x)
main_df$JOB <- factor(main_df$JOB, levels = c("Blue Collar", "Clerical",
                                              "Doctor", "Home Maker","Lawyer",
                                              "Manager", "Professional", "Student"))
x <- main_df$MSTATUS
main_df$MSTATUS <- case_match(x, "z_No" ~ "No", .default = x)
main_df$MSTATUS <- factor(main_df$MSTATUS, levels = c("No", "Yes"))
x <- main_df$RED_CAR
main_df$RED_CAR <- case_match(x, "no" ~ "No", "yes" ~ "Yes", .default = x)
main_df$RED_CAR <- factor(main_df$RED_CAR, levels = c("Yes", "No"))
levels(main_df$REVOKED) <- c("Yes", "No")
x <- main_df$SEX
main_df$SEX <- case_match(x, "M" ~ "Male", "z_F" ~ "Female", .default = x)
main_df$SEX <- factor(main_df$SEX, levels = c("Male", "Female"))
x <- main_df$URBANICITY
main_df$URBANICITY <- case_match(x, "Highly Urban/ Urban" ~ "Urban",
                                 "z_Highly Rural/ Rural" ~ "Rural", .default = x)
main_df$URBANICITY <- factor(main_df$URBANICITY, levels = c("Rural", "Urban"))

```

We then split the data into a train and test set. 

```{r }
set.seed(202)
rows <- sample(nrow(main_df))
main_df <- main_df[rows, ]
sample <- sample(c(TRUE, FALSE), nrow(main_df), replace=TRUE,
                 prob=c(0.7,0.3))
train_df <- main_df[sample, ]
test_df <- main_df[!sample, ]

```

We impute missing data in the train and test sets using the `mice` package for five numeric variables (`AGE`, `INCOME`, `YOJ`, `HOME_VAL`, and `CAR_AGE`) and one categorical variable (`JOB`). For the numeric variables, we use the package's `pmm` (predictive mean matching) method, and for the categorical variable, we use the package's `polyreg` (polytomous, i.e. multinomial, logistic regression) method.

```{r imputation, warning = FALSE, message = FALSE}
col_classes <- unlist(lapply(train_df, class))
missing <- c("AGE", "INCOME", "YOJ", "HOME_VAL", "CAR_AGE", "JOB")
x <- names(col_classes)
not_missing <- x[!x %in% missing]
#Since the imputation process is a little slow, we only do the imputations once, save the results as .csv files once, and load those .csv files moving forward, making sure the levels of the factors stay the same. 
if (file.exists("train_df_imputed.csv") & file.exists("test_df_imputed.csv")){
    train_df_imputed <- read.csv("train_df_imputed.csv", na.strings = "",
                                 colClasses = col_classes)
    test_df_imputed <- read.csv("test_df_imputed.csv", na.strings = "",
                                 colClasses = col_classes)
}else{
    #Start with train_df
    init = mice(train_df, maxit=0) 
    meth = init$method
    predM = init$predictorMatrix
    
    #Skip variables without missing data
    meth[not_missing] = ""
    
    #Set different imputation methods for each of the variables with missing data
    meth[c("AGE")] = "pmm" #Predictive mean matching
    meth[c("INCOME")] = "pmm"
    meth[c("YOJ")] = "pmm"
    meth[c("HOME_VAL")] = "pmm"
    meth[c("CAR_AGE")] = "pmm"
    meth[c("JOB")] = "polyreg" #Polytomous (multinomial) logistic regression
    
    #Impute
    imputed = mice(train_df, method=meth, predictorMatrix=predM, m=5,
                   printFlag = FALSE)
    train_df_imputed <- complete(imputed)
    write.csv(train_df_imputed, "train_df_imputed.csv", row.names = FALSE,
              fileEncoding = "UTF-8")
    
    #Repeat for test_df
    init = mice(test_df, maxit=0) 
    meth = init$method
    predM = init$predictorMatrix
    meth[not_missing] = ""
    meth[c("AGE")] = "pmm"
    meth[c("INCOME")] = "pmm"
    meth[c("YOJ")] = "pmm"
    meth[c("HOME_VAL")] = "pmm"
    meth[c("CAR_AGE")] = "pmm"
    meth[c("JOB")] = "polyreg"
    imputed = mice(test_df, method=meth, predictorMatrix=predM, m=5,
                   printFlag = FALSE)
    test_df_imputed <- complete(imputed)
    write.csv(test_df_imputed, "test_df_imputed.csv", row.names = FALSE,
              fileEncoding = "UTF-8")
}

#Make sure the levels stay the same
levels(train_df_imputed$CAR_TYPE) <- levels(main_df$CAR_TYPE)
levels(train_df_imputed$EDUCATION) <- levels(main_df$EDUCATION)
levels(train_df_imputed$JOB) <- levels(main_df$JOB)
levels(train_df_imputed$MSTATUS) <- levels(main_df$MSTATUS)
levels(train_df_imputed$RED_CAR) <- levels(main_df$RED_CAR)
levels(train_df_imputed$REVOKED) <- levels(main_df$REVOKED)
levels(train_df_imputed$SEX) <- levels(main_df$SEX)
levels(train_df_imputed$URBANICITY) <- levels(main_df$URBANICITY)
levels(test_df_imputed$CAR_TYPE) <- levels(main_df$CAR_TYPE)
levels(test_df_imputed$EDUCATION) <- levels(main_df$EDUCATION)
levels(test_df_imputed$JOB) <- levels(main_df$JOB)
levels(test_df_imputed$MSTATUS) <- levels(main_df$MSTATUS)
levels(test_df_imputed$RED_CAR) <- levels(main_df$RED_CAR)
levels(test_df_imputed$REVOKED) <- levels(main_df$REVOKED)
levels(test_df_imputed$SEX) <- levels(main_df$SEX)
levels(test_df_imputed$URBANICITY) <- levels(main_df$URBANICITY)

```

We confirm there are no longer any missing values in the train or test datasets. 

```{r na_check}
x <- sapply(train_df_imputed, function(x) sum(is.na(x)))
y <- sapply(test_df_imputed, function(x) sum(is.na(x)))
sum(x, y) == 0

```

We reduce the scale of the `INCOME` and `HOME_VAL` variables to thousands of dollars so the figures will be more readable when visualized. The replacement variables are `INCOME_THOU` AND `HOME_VAL_THOU`.

```{r }
drop <- c("INCOME", "HOME_VAL")
train_df_imputed <- train_df_imputed |>
    mutate(INCOME_THOU = INCOME / 1000,
           HOME_VAL_THOU = HOME_VAL / 1000) |>
    select(-all_of(drop))
test_df_imputed <- test_df_imputed |>
    mutate(INCOME_THOU = INCOME / 1000,
           HOME_VAL_THOU = HOME_VAL / 1000) |>
    select(-all_of(drop))
    
```

We take a look at the distributions for our imputed variables to see if the distributions of these variables in the train and test sets differ from what we originally observed or between sets. 

First, we examine the five numeric variables we imputed.

```{r imp_dist_check1}
missing <- c("AGE", "INCOME_THOU", "YOJ", "HOME_VAL_THOU", "CAR_AGE", "JOB")
job <- c("JOB")
keep <- missing[!missing %in% job]
imp_train_num <- train_df_imputed |>
    select(all_of(keep)) |>
    mutate(Set = "Train")
imp_test_num <- test_df_imputed |>
    select(all_of(keep)) |>
    mutate(Set = "Test")
imp_num <- imp_train_num |>
    bind_rows(imp_test_num)
imp_num_pivot <- imp_num |>
    pivot_longer(!Set, names_to = "Variable", values_to = "Value")
p4 <- imp_num_pivot |>
    ggplot(aes(x = Value)) +
    geom_density(fill = "lightblue", color = "black") +
    labs(y = "Density") +
    facet_grid(rows = vars(Set), cols = vars(Variable),
               switch = "y", scales = "free_x")
p4

```

The distributions in the train and test sets for the five imputed numeric variables are all similar to each other, and none of them are dissimilar from the distributions of the original data.

Next we look at the single categorical variable we imputed. 

```{r imp_dist_check2, warning = FALSE, message = FALSE}
imp_train_pivot_cat <- train_df_imputed |>
    select(all_of(missing)) |>
    pivot_longer(cols = all_of(job),
                 names_to = "Variable",
                 values_to = "Value") |>
    group_by(Variable, Value) |>
    summarize(Count = n()) |>
    mutate(Set = "Train")
imp_test_pivot_cat <- test_df_imputed |>
    select(all_of(missing)) |>
    pivot_longer(cols = all_of(job),
                 names_to = "Variable",
                 values_to = "Value") |>
    group_by(Variable, Value) |>
    summarize(Count = n()) |>
    mutate(Set = "Test")
imp_pivot_cat <- imp_train_pivot_cat |>
    bind_rows(imp_test_pivot_cat)
p5 <- imp_pivot_cat |>
    ggplot(aes(x = Value, y = Count)) +
    geom_col(fill = "lightblue", color = "black") +
    labs(x = "Job") + 
    coord_flip() +
    facet_wrap(vars(Set), ncol = 2)
p5

```

The distributions in the train and test sets for the single imputed categorical variable are similar to each other, and the rankings of most frequent to least frequent occupation here are similar to the rankings of the original distribution. We note that the "Professional" and "Manager" occupations are more tied in the rankings here than they were in the original distribution, however. 

### Build Models



### Select Models



### Appendix: Report Code

Below is the code for this report to generate the models and charts above.

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```

---
title: "DATA 621 - HW5"
author: "Andrew Bowen, Glen Davis, Shoshana Farber, Joshua Forster, Charles Ugiagbe"
date: "2023-11-27"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Homework 5 - Count Regression

```{r packages, warning=FALSE, message = FALSE}
library(tidyverse)
library(DataExplorer)
library(knitr)
library(mice)
library(cowplot)
library(scales)
library(MASS)
library(glue)
library(corrplot)
library(naniar)
library(car)
library(finalfit)
library(pscl)
library(faraway)
library(vcd)
```


```{r theme,echo=F}
cur_theme <- theme_set(theme_classic())

```

### Data Exploration

The dataset to be used in this analysis involves sales of over 12,000 different types of commercially available wine. There are 12,795 records across the training set with a response variable `TARGET` that indicates the number of sample cases purchased by wine distribution companies. It is generally more appropriate to use poisson or negative binomial regression methods to predict a discrete dependent variable and different variations will be explored later in the analysis. 

Below is a short description of all the variables of interest in the data set, including these response variables:

```{r load_input, echo=F}
git_link = 'https://raw.githubusercontent.com/andrewbowen19/businessAnalyticsDataMiningDATA621/main/data/wine_training_data.csv'
input_df <- read_csv(git_link,show_col_types = FALSE)
```

|VARIABLE NAME|DEFINITION|
|--|--|
|`INDEX`|Identification Variable|
|`TARGET`|Number of Cases Purchased|
|`ACIDINDEX`|Proprietary method of testing total acidity of wine by using a weighted average|
|`ALCOHOL`|Alcohol Content|
|`CHLORIDES`|Chloride content of wine|
|`CITRICACID`|Citric Acide Content|
|`DENSITY`|Density of Wine|
|`FIXEDACIDITY`|Fixed Acidity of Wine|
|`FREESULFURDIOXIDE`|Sulfur Dioxide Content of Wine|
|`LABELAPPEAL`|Marketing Score indicating the appeal of label design for consumers. High numbers suggest customers like the label design. Negative numbers suggest customers don't like the design.|
|`RESIDUAL SUGAR`|Residual Sugar of Wine|
|`STARS`|Win rating by a team of experts|
|`SULPHATES`|Sulfate content of Wine|
|`TOTLASULFURDIOXIDE`|Total Sulfur Dioxide of Wine|
|`VOLATILEACIDITY`|Volatile Acide content of Wine|
|`PH`|pH Level of Wine|

Let's review the basic numeric distributions from the summary of the dataframe:

```{r summary_input,echo=F}
summary(input_df)
```

All of the variables are numeric included `STARS` although that independent predictor could potentially be treated as a factor given that it's values correspond to a rating scale. Perhaps that will be explored with transformations in the modeling section to determine if that is a better means of prediction. It also has by far the most `NA` values. From reviewing the mean values of all of the variables it appears `CitricAcid` and `FreeSulfurDioxide` have much larger average values than the other potential predictor variables and it'll be interesting to see if that impact coefficients in the models. `ResidualSugar` appears to have some large outlier values given the interquartile range between -2 and 15.9. It is not immediately obvious what a negative value would represent for this column. Further review may be necessary to determine if these are valid data points or distort the model. 

How many observations have negative values (excluding NA values)?


```{r rev_negatives,echo=F}
excl_neg <- c('INDEX','TARGET','pH','STARS')
#neg_df <- input_df |> dplyr::select(-all_of(excl_neg))
neg_cnts <- sapply(input_df |> dplyr::select(-all_of(excl_neg)), function(x) sum(!is.na(x) & x<0))
knitr::kable(t(neg_cnts), format = "simple")
```

Given the proclivity of negative values across multiple columns it would appear to be too widespread an occurrence unless many of the measurements were incorrectly assessed or recorded. It is also not appropriate to change the sign of said observations when it is unclear if that would be a more accurate representation of the data. This may make it a bit harder to explain certain relationships that are encountered in the regression models.

Let's review the frequency of missing values overall across the training dataset:

```{r missing_data, echo=F}
p1 <- plot_missing(input_df, missing_only = TRUE,
                   ggtheme = theme_classic(), title = "Missing Values")

```

As mentioned from the summary of the dataframe `STARS` is missing  about a quarter of all of the observations which will require some form of transformation or modification to be included in a model. `Sulphates` has more null values as well but at 10% may not be as problematic and will likely require some imputation to represent the missing rows. For the other columns that have missing values it is hard to say that samples were taken for these wines for these attributes or maybe something tainted the reading for them.

Review of all variable distributions:

```{r review_distributions,echo=F}

#input_df <- input_df |> dplyr::select(-INDEX)

numeric_train <- input_df[,sapply(input_df, is.numeric)]
par(mfrow=c(4,4))
par(mai=c(.3,.3,.3,.3))
variables <- names(numeric_train)
for (i in 1:(length(variables))) {
  hist(numeric_train[[variables[i]]], main = variables[i], col = "lightblue")
}

```

The vast majority of predictor variables appear to have a fairly normal approximation although `ResidualSugar`, `Chlorides`,`FreeSulfurDioxide`,`CitricAcid` have some minor skew in their distributions. `AcidIndex` appears to more closely resemble a poisson or maybe exponential distribution and the skew may require further transformations. As discussed previously, `STARS` given it's more discrete values in nature is skewed, but likely should not be treated as a numeric input. `LabelAppeal` does seem to be discrete as well which makes sense given its perhaps a standardized marketing score based on consumer feedback. 

```{r boxplot,warning = FALSE,echo=F}
gather_df <- input_df %>% dplyr::select(-INDEX) %>%
  gather(key = 'variable', value = 'value')
ggplot(gather_df, aes(variable, value)) + 
  geom_boxplot() + 
  facet_wrap(. ~variable, scales='free', ncol=4)
```

Most of the variables appear to be highly concentrated in the center, in line with the distributions shown in the histograms, which might cause some of the outlier points per a Boxplot to skew the model. It is not immediately obvious if one dimension will impact the model or if perhaps some of these predictors will not be significant within the regression.

The response variable appears to have a large amount of zero values, which requires some further review:

```{r target_dist, echo=F}

ggplot(input_df, aes(x = TARGET, y = after_stat(density))) +
  geom_histogram(binwidth = 1, fill = 'lightblue', color = 'black', alpha = 0.7, 
                 aes(color = "Histogram")) +
  geom_line(aes(y = dpois(TARGET, mean(TARGET), log = FALSE), color = "Poisson"), 
            linewidth = 1) +
  geom_line(aes(y = dnbinom(x = TARGET, size = 1, prob = 0.2), color = "Negative Binomial"), 
            linewidth = 1) +
  geom_line(aes(y = dnorm(x = TARGET, mean = mean(TARGET), sd = sd(TARGET), log = FALSE), color = "Normal"), linewidth = 1) +
  labs(title = 'Target Histogram and Distribution Overlay',
       x = 'TARGET',
       y = 'Density') +
  scale_color_manual(values = c( "red", "blue", "orange"),
                     labels = c("Negative Binomial","Normal","Poisson")) +
  guides(color = guide_legend(title = "Distributions")) +
  theme_minimal()


```

One key assumption when using poisson regression models is that the response is expected to mirror a poisson distribution which has been plotted on the graph above. An attempt will be made to use a standard poisson model despite the fact that there are many zero values from this distribution. It may be necessary to incorporate zero-inflated modeling techniques to occur for this pattern in the data to improve the accuracy/performance of our regression models. The negative binomial function seems to be able to account for the frequency of zero values in the `TARGET` response, but it is not capturing the distribution of the remaining predicted cases very well. The normal distribution was also included to compare against the poisson given that at higher $\lambda$ values these two distributions tend to converge and in this case are not that substantially different from one another when evaluating the training dataset.

Reviewing $\lambda$ for the poisson distribution:

```{r pois_lambda,echo=F}
print(glue("The response mean: {mean(input_df$TARGET)} and variance: {var(input_df$TARGET)}"))
```

Another important assumption for poisson models relates to the fact that the $\lambda$ parameter which is a critical input for the poisson distribution expects that the  mean and variance must be equal. In practice this is nearly impossible and although there is some overdispersion for the response it is not bad enough to disqualify using this method. 

Let's analyze the correlation relationship among predictors and the response:

```{r check-multicolin, echo=F,out.width='90%'} 

corrplot(cor(na.omit(input_df)),method="color",diag=FALSE,type="lower",addCoef.col = "black",number.cex=0.60)

```

After excluding the NA values, there are weak correlation values across the board except for `TARGET`,`STARS` and `LabelAppeal`. This is not the most encouraging evidence that the predictor variables provided in the dataset are going to be that successful in modeling the response.

One hypothesis for the apparent lack of linear relationships between variables and the response may be driven off the substantial number of zero values in the response. A potential guess for missing `TARGET` values might be the fact that many more brands of wine exist that are minimally distributed on a commercial scale.

What is the relationship among the variables when excluding the zero response values?

```{r non_zero_corr,echo=F}
nonzero_df <- na.omit(input_df) |> filter(TARGET!=0) 
corrplot(cor(nonzero_df),method="color",diag=FALSE,type="lower",addCoef.col = "black",number.cex=0.60)
```

There aren't many changes across the correlations after excluding the zero response rows. `LabelAppeal` appears to have a stronger linear relationship with the `TARGET` and `STARS`. Two different acidity (`AcidIndex` and `FixedAcidity`) inputs also have more correlation than before although it is still a fairly weak relationship.


### Data Preparation

As discussed in the exploratory section, the `STARS` variable appears to be something that can be treated as a factor. The number of NA values would likely indicate that it is not a rated wine by the experts and much less likely to have sample cases purchased by distributors. The missing values will be updated to zero to mirror the scale of this discrete predictor.

```{r prep_stars, echo=F}
mod_df <- input_df |> dplyr::select(-INDEX) |> mutate(STARS=as.factor(ifelse(is.na(STARS),0,STARS)))
```

The remaining columns with missing values are far less clear to evaluate and will need a more standardized imputation method:

One more comprehensive method to evaluate if the data is Missing Completely at random (MCAR) is running Little's test although this is unlikely to be the case for most real world datasets.

```{r missing_completely_at_random}
littles_test <- input_df |>
    mcar_test()
knitr::kable(littles_test, format = "simple")

```

The zero in the p-value indicates that the missing values across these columns are not in fact MCAR.

Let's review if the NA values appear in similar observations:

```{r missing_values_map,echo=F}
na_cols <- c("pH", "ResidualSugar", "Chlorides", "FreeSulfurDioxide",'Alcohol','TotalSulfurDioxide','Sulphates')
na_col_review <- mod_df |>
    dplyr::select(all_of(na_cols)) |>
    missing_plot()
na_col_review


```

The plot above highlights via a heat map all of the individuals rows where the independent predictors have NA values (shaded in light blue). It's a bit challenging to discern where observations have more than one null value, but there is definitely some overlap across records.

```{r row_index,warning=FALSE,echo=F}
na_freq <- mod_df |> dplyr::select(all_of(na_cols)) |> mutate(row_index=1:nrow(mod_df)) |> pivot_longer(cols=na_cols,names_to='header') |> filter(is.na(value)) |> group_by(row_index) |> summarise(num_na=n())

ggplot(na_freq,aes(x=num_na)) +
    geom_bar() +
    labs(x='Number of NA columns',y='Number of Rows',title='Distribution of NA Values across columns')

print(glue("Only {pull(na_freq |> filter(num_na>1)|>summarise(total=n()))} rows have more than 1 column with NA, which is only {format(round(pull(na_freq |> filter(num_na>1)|>summarise(total=n())/nrow(na_freq))*100,2),nsmall=2)}%"))

```

It does not appear that many observations have more than 1 NA value and the heatmap was a little bit less clear in helping us identify overlapping columns.

Let's split the data into train and test sets to provide some observations to evaluate the selected model as a holdout set:

```{r train_test_split,echo=F}
set.seed(19)
rows <- sample(nrow(mod_df))
sample <- sample(c(TRUE, FALSE), nrow(mod_df), replace=TRUE,
                 prob=c(0.7,0.3))
train_df <- mod_df[sample, ]
test_df <- mod_df[!sample, ]

```

We are going to compare two methods of imputation to see how the results impact the regression models. Given the central concentration of many of the predictors it may be enough to apply the averages to the missing values to estimate them for inclusion in the modeling. The first of which will be replacing the missing values with averages except for `STARS` which we will add a zero value:

```{r avg_impute,echo=F}
avg_impute_train_df <- train_df
avg_impute_test_df <- test_df

avg_impute_train_df$STARS[is.na(avg_impute_train_df$STARS)] <- 0
avg_impute_train_df$STARS <- as.factor(avg_impute_train_df$STARS)
avg_impute_train_df$Sulphates[is.na(avg_impute_train_df$Sulphates)] <- mean(avg_impute_train_df$Sulphates, na.rm = TRUE)
avg_impute_train_df$TotalSulfurDioxide[is.na(avg_impute_train_df$TotalSulfurDioxide)] <- mean(avg_impute_train_df$TotalSulfurDioxide, na.rm = TRUE)
avg_impute_train_df$FreeSulfurDioxide[is.na(avg_impute_train_df$FreeSulfurDioxide)] <- mean(avg_impute_train_df$FreeSulfurDioxide, na.rm = TRUE)
avg_impute_train_df$Alcohol[is.na(avg_impute_train_df$Alcohol)] <- mean(avg_impute_train_df$Alcohol, na.rm = TRUE)
avg_impute_train_df$Chlorides[is.na(avg_impute_train_df$Chlorides)] <- mean(avg_impute_train_df$Chlorides, na.rm = TRUE)
avg_impute_train_df$ResidualSugar[is.na(avg_impute_train_df$ResidualSugar)] <- mean(avg_impute_train_df$ResidualSugar, na.rm = TRUE)
avg_impute_train_df$pH[is.na(avg_impute_train_df$pH)] <- mean(avg_impute_train_df$pH, na.rm = TRUE)
avg_impute_train_df$FixedAcidity[is.na(avg_impute_train_df$FixedAcidity)] <- mean(avg_impute_train_df$FixedAcidity, na.rm = TRUE)

avg_impute_test_df$STARS[is.na(avg_impute_test_df$STARS)] <- 0
avg_impute_test_df$STARS <- as.factor(avg_impute_test_df$STARS)
avg_impute_test_df$Sulphates[is.na(avg_impute_test_df$Sulphates)] <- mean(avg_impute_test_df$Sulphates, na.rm = TRUE)
avg_impute_test_df$TotalSulfurDioxide[is.na(avg_impute_test_df$TotalSulfurDioxide)] <- mean(avg_impute_test_df$TotalSulfurDioxide, na.rm = TRUE)
avg_impute_test_df$FreeSulfurDioxide[is.na(avg_impute_test_df$FreeSulfurDioxide)] <- mean(avg_impute_test_df$FreeSulfurDioxide, na.rm = TRUE)
avg_impute_test_df$Alcohol[is.na(avg_impute_test_df$Alcohol)] <- mean(avg_impute_test_df$Alcohol, na.rm = TRUE)
avg_impute_test_df$Chlorides[is.na(avg_impute_test_df$Chlorides)] <- mean(avg_impute_test_df$Chlorides, na.rm = TRUE)
avg_impute_test_df$ResidualSugar[is.na(avg_impute_test_df$ResidualSugar)] <- mean(avg_impute_test_df$ResidualSugar, na.rm = TRUE)
avg_impute_test_df$pH[is.na(avg_impute_test_df$pH)] <- mean(avg_impute_test_df$pH, na.rm = TRUE)
avg_impute_test_df$FixedAcidity[is.na(avg_impute_test_df$FixedAcidity)] <- mean(avg_impute_test_df$FixedAcidity, na.rm = TRUE)

```

Let's confirm that all the missing values were populated after applying the averages:

```{r impute_avg_check,echo=F}
x <- sapply(avg_impute_test_df, function(x) sum(is.na(x)))
y <- sapply(avg_impute_train_df, function(x) sum(is.na(x)))
sum(x, y) == 0
```

As anticipated this imputation method brings in all of the observations for the models that we select in the next section.

Alternatively, we will use the MICE package to fill the NA values:

```{r imputation_mice,echo=F}

df_names <- colnames(train_df)
non_na_cols <- df_names[!df_names %in% na_cols]

if (file.exists("imputed_wine_test_df.csv") & file.exists("imputed_wine_train_df.csv")){
    train_df_imputed_input <- read.csv("imputed_wine_train_df.csv", na.strings = "")
    test_df_imputed_input <- read.csv("imputed_wine_test_df.csv", na.strings = "")
    train_df_imputed_input$STARS <- as.factor(train_df_imputed_input$STARS)
    test_df_imputed_input$STARS <- as.factor(test_df_imputed_input$STARS)
}else{
        #Train Data Imputation First
    init = mice(train_df, maxit=0) 
    meth = init$method
    predM = init$predictorMatrix
    
    
    meth[non_na_cols] = ""
    
    meth[c("pH")] = "pmm"
    meth[c("ResidualSugar")] = "pmm"
    meth[c("Chlorides")] = "pmm"
    meth[c("FreeSulfurDioxide")] = "pmm"
    meth[c("Alcohol")] = "pmm"
    meth[c("TotalSulfurDioxide")] = "pmm"
    meth[c("Sulphates")] = "pmm"

    imputed_train = mice(train_df, method=meth, predictorMatrix=predM, m=5,
                   printFlag = FALSE)
    train_df_imputed <- complete(imputed_train)
    write.csv(train_df_imputed, "imputed_wine_train_df.csv", row.names = FALSE,
              fileEncoding = "UTF-8")
    
    #Repeat for test_df
    init = mice(test_df, maxit=0) 
    meth = init$method
    predM = init$predictorMatrix
    meth[non_na_cols] = ""
    meth[c("pH")] = "pmm"
    meth[c("ResidualSugar")] = "pmm"
    meth[c("Chlorides")] = "pmm"
    meth[c("FreeSulfurDioxide")] = "pmm"
    meth[c("Alcohol")] = "pmm"
    meth[c("TotalSulfurDioxide")] = "pmm"
    meth[c("Sulphates")] = "pmm"
    imputed_test = mice(test_df, method=meth, predictorMatrix=predM, m=5,
                   printFlag = FALSE)
    imputed_test_df <- complete(imputed_test)
    write.csv(imputed_test_df, "imputed_wine_test_df.csv", row.names = FALSE,
              fileEncoding = "UTF-8")

}

```

Let's confirm that the imputation filled all the null values for MICE:

```{r impute_mice_check,echo=F}
x <- sapply(train_df_imputed_input, function(x) sum(is.na(x)))
y <- sapply(test_df_imputed_input, function(x) sum(is.na(x)))
sum(x, y) == 0
```

As expected the MICE imputed estimate values for all the missing fields and we can now include all observations within the model.


Let's review the distributions of the train and test imputed datasets to confirm they remain similar to the initial unmodified dataset:

```{r review_imputedistrib, warning = FALSE, message = FALSE}
#need to fix for spacing
sub_vars <- c('Chlorides','pH','Sulphates')
other_nas <- c('ResidualSugar','FreeSulfurDioxide','Alcohol','TotalSulfurDioxide')
impute_train_input <- train_df_imputed_input |>
    dplyr::select(all_of(sub_vars)) |>
    mutate(Set = "Train")
impute_test_input <- test_df_imputed_input |>
    dplyr::select(all_of(sub_vars)) |>
    mutate(Set = "Test")
impute_both <- impute_train_input |>
    bind_rows(impute_test_input)
impute_pivot <- impute_both |>
    pivot_longer(!Set, names_to = "Variable", values_to = "Value")
impute_plot <- impute_pivot |>
    ggplot(aes(x = Value)) +
    geom_density(fill = "lightblue", color = "black") +
    labs(y = "Density") +
    facet_grid(rows = vars(Set), cols = vars(Variable),
               switch = "y", scales = "free_x")
impute_plot

```

These columns were separated as they had higher density points distorting the remainder of predictors that had NA values in the density plots. Across train and test sets it still appears to be similarly distributed and not substantially different from the initial input data.

```{r review_other_imputedistrib, warning = FALSE, message = FALSE}


impute_train_input2 <- train_df_imputed_input |>
    dplyr::select(all_of(other_nas)) |>
    mutate(Set = "Train")
impute_test_input2 <- test_df_imputed_input |>
    dplyr::select(all_of(other_nas)) |>
    mutate(Set = "Test")
impute_both2 <- impute_train_input2 |>
    bind_rows(impute_test_input2)
impute_pivot2 <- impute_both2 |>
    pivot_longer(!Set, names_to = "Variable", values_to = "Value")
impute_plot2 <- impute_pivot2 |>
    ggplot(aes(x = Value)) +
    geom_density(fill = "lightblue", color = "black") +
    labs(y = "Density") +
    facet_grid(rows = vars(Set), cols = vars(Variable),
               switch = "y", scales = "free_x")
impute_plot2

```

The remaining columns that had imputations also mirror one another in the train and test sets as well as the initial distributions. We can proceed with additional review of other variables that need transformations.

For the most part the predictors themselves seemed approximately normal, but there is one additional variable, `AcidIndex` that remains skewed that may require further transformation.

```{r box_cox_transform,echo=F}
acid_bc <- boxcox(lm(train_df_imputed_input$AcidIndex ~ 1))
acid_lambda <- acid_bc$x[which.max(acid_bc$y)]

```

The maximum likelihood estimation for this variable was -1.15 although we are bit skeptical that this transformation is necessary or will approximately normalize the distribution of this column.


Let's see what the distribution of `AcidIndex` looks like with the reciprocal transformation $\frac{1}{x}$ :

```{r plot_acidbc,warning=FALSE,echo=F}
train_df_imputed_input |> mutate(bc_acidindex=(AcidIndex ^ round(acid_lambda) - 1)/round(acid_lambda)) %>%
    ggplot(aes(x=bc_acidindex)) +
    geom_histogram() +
    labs(title='Testing Proposed Box Cox (-1) Transformation of AcidIndex',x='AcidIndex Transformed')
```

As expected it does not seem warranted to transform this variable after reviewing the histogram of the transformed predictor given that the data is still fairly sparse although it does have a more bell shape than before it doesn't seem to improve the distribution enough to warrant implementation.


Before proceeding further let's double check the correlations of the target and predictor variables to see how imputation impacted the linear relationships across the columns:

```{r impute_corrplot,echo=F}
corrplot(cor(train_df_imputed_input |> dplyr::select(-STARS)),method="color",diag=FALSE,type="lower",addCoef.col = "black",number.cex=0.60)
```

By modifying the `STARS` into a factor we have excluded it from the correlation plot but would expect it remains one of the stronger correlated predictors. The `LabelAppeal` value seems to have decreased quite a bit although it's relationship is far stronger than most of the other potential predictors. Perhaps it's worth considering modifying this variable into a factor as well to see if that impacts the performance. Depending on model performance this might be something worth exploring. `AcidIndex` has the strongest negative relationship among the predictors based on the imputed observations has a somewhat strengthened relationship with `FixedAcidity`. Semantically the two names of these variables appear as though they should have some relationship, but it is not that strong of a correlation.

One means of prediction using OLS for count data is taking the $ln y$ as a means of replicating the Poisson linear transformation by applying it on the response. This transformation is based on a UC Davis [publication] (https://cameron.econ.ucdavis.edu/racd/simplepoisson.pdf) on count regression. Therefore we will prepare an alternative response variable although some minor modifications are needed since there are many zero values.

Let's review what the Box-Cox method proposes for the transformation:

```{r transform_resp,echo=F}
train_df_imputed_input <- train_df_imputed_input |> mutate(TARGET.TF=ifelse(TARGET==0,TARGET+0.001,TARGET))
target_bc <- boxcox(lm(train_df_imputed_input$TARGET.TF  ~ 1))
target_lambda <- target_bc$x[which.max(target_bc$y)]
target_lambda
```

The Box-Cox preferred transformation appears to show the square root as the optimal transformation, but given the primer on count data from UC-Davis we will apply the $ln y$ instead.

```{r}
train_df_imputed_input <- train_df_imputed_input |> mutate(TARGET.TF=log(ifelse(TARGET==0,0.5,TARGET)))
hist(train_df_imputed_input$TARGET.TF)
```



### Modeling


#### Model POIS:1 - Full Poisson Count Model with All Missing Values Imputed using Averages - Reduced via Stepwise AIC Model Selection

We create Model 1: Poisson with all missing values imputed and we perform stepwise model selection to select the model with the smallest AIC value using the `stepAIC()` function from the `MASS` package. 

A summary of Model 1: Poisson below:

```{r pois1,echo=F}
mod1_pois <- glm(TARGET ~ ., family = 'poisson',
                   data = avg_impute_train_df)
mod1_pois <- stepAIC(mod1_pois, trace = 0)
summary(mod1_pois)

```

The AIC of Model 1 is 32072 which appears to be very high and may indicate other problems that have been discussed during the explanatory section. We will assess the predictors more formally after building a zero inflated equivalent as a comparison.

Residual Deviance: Chi-Squared Test:


```{r mod1_chisq,echo=F}
with(mod1_pois, cbind(res.deviance = deviance, df = df.residual,
  p = pchisq(deviance, df.residual, lower.tail=FALSE)))
```

This test measures the goodness of fit of the poisson model and an ideal case is if when the test returns a non-significant p-value. It is designed to compare the expected counts (response) returned from the model to the observed values and if the model did as expected there would not be a statistically significant difference between the two values. Therefore, this would indicate that the data does not fit the model well and given that all the available predictor variables were initially included that we violated the linear distribution of the Poisson and there was more dispersion than allowed for the Poisson.

Let's plot the log scaled version of the response and the predicted number of cases:

```{r log_resp_diag,echo=F}
e_y <- predict(mod1_pois,type="response")
plot(log(e_y+1),log(avg_impute_train_df$TARGET+1),xlim=c(0,4),xlab='Log Scaled Estimated Cases',ylab='Log Scaled Observed Cases')
title(main='Comparing Estimated vs Observed number of cases of wine sold')
```

Consistent with the Chi-Squared Test it appears there are too many observed values of zero to fit into the Poisson distribution. This plot is designed to identify where the estimated number of cases on the x axis diverges from the actual values particularly emphasizing the concentration of zero values that a poisson distribution is unable to accurately estimate.

The half-norm plot of the residuals and a plot of Cook's D statistic are two different ways of identifying outliers:

The half norm plot assesses the distribution using quantiles similar to the qqplot

```{r mod1_outlier,echo=F}
faraway::halfnorm(residuals(mod1_pois))

```

The half-norm plot highlights two points at the top right of the graph, but does not appear to show substantial divergence to warrant classifying observations 624 and 3467 as outliers.

```{r mod1_influence,echo=F}
car::influencePlot(mod1_pois)
```

The influence plot appears that point 1130 is a leverage point; however, it doesn't cross the -2 boundary that would classify it as an outlier for the model. There are other points listed below the graph as well that have much larger standardized residual values but don't have divergent case predictions and therefore do not appear to be outliers.

```{r mod1_dev,echo=F}
res <- residuals(mod1_pois, type="deviance")
#abline(h=0, lty=2)
qqnorm(res)
qqline(res)
```

The QQplot would seem to indicate that the deviance residuals are diverging from the expected distribution.

We check for over/underdispersion of the model to see if there are any additional violations of assumptions.

Let's calculate the dispersion from the model:

```{r pois_dispersion,echo=F}
## Check for over/underdispersion in the model
E2 <- resid(mod1_pois, type = "pearson")
N  <- nrow(train_df_imputed_input)
p  <- length(coef(mod1_pois))   
sum(E2^2) / (N - p)
```


It appears there is a decent amount of under-dispersion in this model as the ideal case would return a value of 1, which may limit our abilities to reliably use Poisson Regression for this count data response consistent with other diagnostic methods conducted. Generally, the concern to evaluate Poisson regression is assessing overdispersion violations, but in this case the zero values are driving this issue. It does appear that the frequency of zero values does not follow a standard Poisson distribution

#### Model POIS:2 - Zero Inflated Poisson Count Model (ZIP) with All Missing Values Imputed with MICE

```{r zero_inflated_poisson,echo=F}
mod2_zip <- pscl::zeroinfl(formula=TARGET ~ STARS + LabelAppeal + Alcohol + AcidIndex| STARS + LabelAppeal,data=train_df_imputed_input,dist='poisson')


summary(mod2_zip)
```

This predictors included in this model were based on the items that were significant to the model rather than including a full one with all of the variables. There is some discussion in statistical publications around using robust methods to estimate the standard error when using zero inflation techniques, but we will review other diagnostic methods to evaluate this model although it does not typically impact coefficients. The logit model used the strongest predictors (`STARS` and `LabelAppeal`) to identify the instances when no cases were expected to be sold based on the expectation that negative customer evaluation and missing expert reviews would prompt distributors to not purchases cases of these wines. Somewhat logically the higher `STARS` ratings did not closely align with zero cases and these values are not significant in the logistic piece of the model. When reviewing the coefficients in the binomial count model it makes sense that the negative impact of additional `STARS` increases with each additional improvement in the rating.

Let's run a Chi-Squared Test to evaluate the goodness of fit:

```{r zip_chisq,echo=F}
m2null <- update(mod2_zip, . ~ 1)

pchisq(2 * (logLik(mod2_zip) - logLik(m2null)), df = 7, lower.tail = FALSE)
```

A statistically significant result in the Chi-Squared Test would seem to indicate that there is a goodness of fit for the ZIP model. The degrees of freedom is based on the number of predictors in the model excluding the intercept (the null model)

```{r mod2_res,echo=F}
plot(residuals(mod2_zip) ~
      fitted(mod2_zip),xlab="Fitted",ylab="Residuals")
abline(h=0)
```

When assessing the fitted vs residuals plot there is some pattern to the residuals and the largest ones still exist for the zero cases examples it would appear. They are still centered around zero.

```{r mod2_qq,echo=F}
res_mod2 <- residuals(mod2_zip,'pearson')
qqnorm(res_mod2)
qqline(res_mod2)

abline(0, 1, col = "red")
```

The residuals primarily follow the QQline for most of the quantiles, but appear to diverge on the higher end and follow a line tracking from zero to one. 

```{r mod2_disp,echo=F}
E2_zip2 <- resid(mod2_zip, type = "pearson")
N_zip2 <- nrow(input_df)
p_zip2 <- length(coef(mod2_zip)) # '+1' is for variance parameter in NB
sum(E2_zip2^2) / (N_zip2 - p_zip2)
```

There is more substantial underdispersion captured in this model than the other poisson regression one and this may be one reason why another model is ultimately selected as we would like to avoid any under/over dispersion that exists.

Let's compare the Akaike Information Criterion for both poisson based models:

```{r aic_pois,echo=F}
AIC(mod1_pois,mod2_zip)
```

While the AIC values are not vastly different it is apparent that the zero inflated Poisson regression model is preferable to the first poisson regression model. This likely has to do with the strong concentration of zero values that exist in the response variable that is accounted for as a separate logistic model in the zero inflated method. 

Let's review both of the models to compare the similarities and differences from the summaries and model coefficients despite some obvious limitations identified with the the first model using standard poisson regression. In terms of coefficients we will begin with the most influential which alternate between `STARS` and `LabelAppeal` although both have appear to have the same p-value between both models. (note that some resources online suggest applying a robust method of convergence to determine the standard error which would influence the significance value). It makes sense that these two predictors are in some ways the biggest drivers for distributors because on paper it would seem the reviews of customers implies sales potential for a given wine type. Irrespective of the chemical makeup of the wines the customers will drive companies to meet that demand.  `Alcohol` and `AcidIndex` were the only other significant predictors in the zero inflated poisson (ZIP) model. Both predictors have the same direction for their coefficients and it appears the `AcidIndex` will decrease the number of sold wine cases. The other main difference between the two is the number of significant predictors based on poisson, which may be based on the fact that the model is attempting to capture the concentration of zero values.


#### Model NB:1 - Negative Binomial Count Model with All Imputed Variables based on Averages

```{r mod3_nb,echo=F}

mod3_nb <- glm.nb(TARGET ~ STARS + LabelAppeal + Alcohol + AcidIndex + VolatileAcidity + FreeSulfurDioxide + TotalSulfurDioxide,data = avg_impute_train_df)

# summary of results
summary(mod3_nb)

```

The negative binomial distribution can sometimes better approximate dispersion when the mean and variance are not equal to one another which would violate the Poisson distribution and related assumptions for Poisson Count regression. Therefore, we will use most of the same variables as selected in the backward selection aside from the non-significant predictors to try to create a similar comparison aside from the distribution used.

In terms of the strength of the coefficients, the `STARS`,`LabelAppeal`, and `AcidIndex` are the largest predictors in both models and all have the same sign. It makes sense that these are likely the most important independent variables. The remainder of the predictors are somewhat consistent across both variations of the models as well.

One way to assess the negative binomial model is to run a likelihood ratio test to compare Model 3 using the negative binomial family against Model 1 using the Poisson family.

```{r nb_log_like,echo=F}
pchisq(2 * (logLik(mod3_nb) - logLik(mod1_pois)), df = 1, lower.tail = FALSE)
```

Based on this test it would appear that the negative binomial model, Model 3, is more appropriate than Model 1 given the significance of this likelihood value.

```{r mod3_res,echo=F}
plot(residuals(mod3_nb) ~
      fitted(mod3_nb),xlab="Fitted",ylab="Residuals")
abline(h=0)
```

When reviewing the fitted versus residual plot there is some pattern in the residuals which appears to track closely with the `TARGET` shape where even the negative binomial has the largest residuals for many zero cases. The spread overall doesn't make it particularly if the variance is changing although there is clear separation in the plot due to the discrete nature of the response variable.

```{r mod3_qq,echo=F}
res_mod_nb <- residuals(mod3_nb,'pearson')
qqnorm(res_mod_nb)
qqline(res_mod_nb)

```

When using the pearson residuals we can see that for the interquartile range the negative binomial model tracks well with the expected distribution, but ultimately diverges on both tails somewhat similarly to the other ZIP model.

```{r nb_disp,echo=F}
E2_mod3 <- resid(mod3_nb, type = "pearson")
N  <- nrow(input_df)
p_mod3  <- length(coef(mod3_nb)) + 1 # added for variance parameter
sum(E2_mod3^2) / (N - p_mod3)
```


#### Model NB:2 - Zero Inflated Negative Binomial (ZINB) using Imputed values with MICE

Let's try to evaluate the data based on a zero inflated negative binomial:

```{r mod4_zinb,warning=FALSE,echo=F}
mod4_zinb <- pscl::zeroinfl(formula=TARGET ~ STARS + LabelAppeal + Alcohol + AcidIndex| STARS + LabelAppeal,data=train_df_imputed_input,dist='negbin')

summary(mod4_zinb)
```

We tried to be consistent with the zero inflated model for the negative binomial and used the same independent variables as what was identified as significant in zero inflated poisson model. From a review of the summary, the residuals appear to be distributed around zero although there is a slight positive skew as compared to the negative. All of the predictors are also significant which is consistent with the Poisson ZIP Model. The 1-Star ratings appear to be a bit less significant to the model and in the logistic component the higher `STARS` are also insignificant although this makes some sense given higher rated wines would likely have cases ordered.


```{r res_mod4,echo=F}
res_mod4 <- residuals(mod4_zinb,'pearson')
qqnorm(res_mod4)
qqline(res_mod4)

abline(0, 1, col = "red")
```

The QQplot aligns more closely with the distribution line; however, similar to the other distributions there are a lot of points on the right tail that are skewed.

```{r aic_compare,echo=F}
aic_compare2<- AIC(mod1_pois,mod2_zip,mod3_nb,mod4_zinb)
knitr::kable(aic_compare2, format = "simple")
```

When comparing the performance of the models using the AIC statistic, both zero inflated models have a very similar criterion score and the negative binomial model more closely aligns with the poisson model.


#### Model MLR 5: Multiple Linear Regression on All Imputed Variables based on Averages


```{r mod5,echo=F}
mod5_mlr <- lm(TARGET ~ .,data=avg_impute_train_df)
mod5_mlr <- stepAIC(mod5_mlr, trace = 0)
summary(mod5_mlr)
```

When running backward selection on the variables in multiple linear regression we see that two variables (`Density` and `Sulphates`) remain despite being classified as insignificant. In the poisson regression (Model 1) `Sulphates` was also included although `Chlorides` was not in Poisson;however, that does closely align somewhat with the negative binomial model (Model 3). The signs for each coefficient match Model 3 (NB) and the most significant predictors remain `STARS`, `LabelAppeal`, and `AcidIndex`. The F-statistic indicates that the model is better than a pure intercept approach when running MLR.

The residuals appears to be somewhat evenly spread around zero although there is a partial right skew. The $R^2_{adj}$ value shows about 53.47% of the variability can be explained by the variation in the predictors.

Is there any multicollinearity concerns in this model?

```{r mod5_vif,echo=F}
knitr::kable(vif(mod5_mlr), format = "simple")
```

No issues between multicollinearity which is to be expected given the low correlation values across many of the predictors in the correlation plot from the explanatory section.


MLR Diagnostic Plots:

Let's review the diagnostic plots to assess if any assumptions are violated:

```{r mlr_diag,echo=F}
par(mfrow=c(2,2))
plot(mod5_mlr)
```

The Fitted vs Residuals appears to show a slight pattern with the majority of the residuals above zero although this kind of makes sense given the heavy concentration of zeros and the fact that predicted number of cases will never be negative. The QQ plot seems to follow the assumption that the residuals follow a normal distribution and the right skew does not diverge that substantially from the expected case. The Standardized residuals show a clear pattern at each discrete value which is generally one reason why OLS is not encouraged for count data as it does not appear to as if a linear model adequately captures the relationship between the predictors and the response variables. Not a ton of high leverage points that exist although 7699 appears to be close to the Cook's distance statistic.

#### Model MLR 6: Multiple Linear Regression on All Imputed Variables based on MICE and ln y response


```{r mod6,echo=F}
mod6_mlr <- lm(TARGET.TF ~ . - TARGET,data=train_df_imputed_input)
mod6_mlr <- stepAIC(mod6_mlr, trace = 0)
summary(mod6_mlr)
```

Both this variation of the model and model 5 which both use OLS have very similar coefficients for all of the significant variables after running backward selection. They appear to emphasize the importance of `STARS`, `LabelAppeal`, and `AcidIndex` much like the other models. `Alcohol` is less significant in this model though. The $R^2_{adj}$ is slightly lesser than Model 5 at 48.88% and the residual standard error is about half the equivalent in Model 5. The interpretability of the model is certainly reduced when considering the $ln y$ because it is a percentage change in cases of wine given a one unit change in a given predictor.

Is there any multicollinearity concerns in this version of the model?

```{r mod6_vif,echo=F}
knitr::kable(vif(mod6_mlr), format = "simple")
```

No issues between multicollinearity which is to be expected given the low correlation values across many of the predictors in the correlation plot from the explanatory section.


MLR Diagnostic Plots:

Let's review the diagnostic plots to assess if any assumptions are violated:

```{r mlr6_diag,echo=F}
par(mfrow=c(2,2))
plot(mod6_mlr)
```

The fitted residuals are not evenly distributed around zero although the variance does not appear to change across the range of the predicted values. The QQ plot for the most part conforms with the expected normal distribution of errors except for a left skew which is likely driven off of all the transformed zero values that were in the `TARGET` variable. The scaled version of the fitted versus residuals shows a clear pattern that would violate the assumption that there is a linear relationship between the response and the predictors. It's not clear that it is acceptable to use this model for any inference going forward due to the violations of key assumptions.


```{r mlr_aic,echo=F}
AIC(mod_5_mlr,mod6_mlr)
```

Despite the fact that each MLR model has a lower AIC value it would be unwise to use these models further given the violation of assumptions as previously mentioned when reviewing the diagnostic plots in prior sections.

#### Select Models

Based on the fact that multiple models that were tested in the prior section had somewhat serious flaws, we are going to evaluate the zero inflated models against the hold out set in order to make a final selection of the "best" model. It still appears as though there are some challenges in accurately identifying all of the zero cases scenarios.

```{r pred,echo=F}
zip_pred <- predict(mod2_zip,test_df_imputed_input,type='count')
zinb_pred <- predict(mod4_zinb,test_df_imputed_input,type='count')
holdout_zip <- as.data.frame(cbind(test_df_imputed_input,zip_pred,zinb_pred))
head(holdout_zip)
holdout_zip |> group_by(TARGET) |> summarize(total=n(),round(zip_pred,0))
```


```{r plot_pred,echo=F}
ggplot(holdout_zip,aes(x=zip_pred,y=TARGET)) +
    geom_point()
```


---
title: "DATA 621 - HW5"
author: "Andrew Bowen, Glen Davis, Shoshana Farber, Joshua Forster, Charles Ugiagbe"
date: "2023-11-27"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Homework 5 - Count Regression

```{r packages, warning=FALSE, message = FALSE}
library(tidyverse)
library(DataExplorer)
library(knitr)
library(mice)
library(cowplot)
library(scales)
library(MASS)
library(glue)
library(corrplot)
library(naniar)
library(car)
library(finalfit)
library(pscl)
library(faraway)
```


```{r theme}
cur_theme <- theme_set(theme_classic())

```

### Data Exploration

The dataset to be used in this analysis involves sales of over 12,000 different types of commercially available wine. There are 12,795 records across the training set with a response variable `TARGET` that indicates the number of sample cases purchased by wine distribution companies. It is generally more appropriate to use poisson or negative binomial regression methods to predict a discrete dependent variable and different variations will be explored later in the analysis. 

Below is a short description of all the variables of interest in the data set, including these response variables:

```{r load_input, echo=F}
git_link = 'https://raw.githubusercontent.com/andrewbowen19/businessAnalyticsDataMiningDATA621/main/data/wine_training_data.csv'
input_df <- read_csv(git_link,show_col_types = FALSE)
```

|VARIABLE NAME|DEFINITION|
|--|--|
|`INDEX`|Identification Variable|
|`TARGET`|Number of Cases Purchased|
|`ACIDINDEX`|Proprietary method of testing total acidity of wine by using a weighted average|
|`ALCOHOL`|Alcohol Content|
|`CHLORIDES`|Chloride content of wine|
|`CITRICACID`|Citric Acide Content|
|`DENSITY`|Density of Wine|
|`FIXEDACIDITY`|Fixed Acidity of Wine|
|`FREESULFURDIOXIDE`|Sulfur Dioxide Content of Wine|
|`LABELAPPEAL`|Marketing Score indicating the appeal of label design for consumers. High numbers suggest customers like the label design. Negative numbers suggest customers don't like the design.|
|`RESIDUAL SUGAR`|Residual Sugar of Wine|
|`STARS`|Win rating by a team of experts|
|`SULPHATES`|Sulfate content of Wine|
|`TOTLASULFURDIOXIDE`|Total Sulfur Dioxide of Wine|
|`VOLATILEACIDITY`|Volatile Acide content of Wine|
|`PH`|pH Level of Wine|

Let's review the basic numeric distributions from the summary of the dataframe:

```{r summary_input,echo=F}
summary(input_df)
```

All of the variables are numeric included `STARS` although that independent predictor could potentially be treated as a factor given that it's values correspond to a rating scale. Perhaps that will be explored with transformations in the modeling section to determine if that is a better means of prediction. It also has by far the most `NA` values. From reviewing the mean values of all of the variables it appears `CitricAcid` and `FreeSulfurDioxide` have much larger average values than the other potential predictor variables and it'll be interesting to see if that impact coefficients in the models. `ResidualSugar` appears to have some large outlier values given the interquartile range between -2 and 15.9. It is not immediately obvious what a negative value would represent for this column. Further review may be necessary to determine if these are valid data points or distort the model. 

How many observations have negative values (excluding NA values)?


```{r rev_negatives,echo=F}
excl_neg <- c('INDEX','TARGET','pH','STARS')
#neg_df <- input_df |> dplyr::select(-all_of(excl_neg))
neg_cnts <- sapply(input_df |> dplyr::select(-all_of(excl_neg)), function(x) sum(!is.na(x) & x<0))
knitr::kable(t(neg_cnts), format = "simple")
```

Given the proclivity of negative values across multiple columns it would appear to be too widespread an occurrence unless many of the measurements were incorrectly assessed or recorded. It is also not appropriate to change the sign of said observations we it is unclear if that would be a more accurate representation of the data. This may make it a bit harder to explain certain relationships that are encountered in the regression models.

Let's review the frequency of missing values overall across the training dataset:

```{r missing_data, echo=F}
p1 <- plot_missing(input_df, missing_only = TRUE,
                   ggtheme = theme_classic(), title = "Missing Values")

```

As mentioned from the summary of the dataframe `STARS` is missing  about a quarter of all of the observations which will require some form of transformation or modification to be included in a model. `Sulphates` has more null values as well but at 10% may not be as problematic and will likely require some imputation to represent the missing rows. For the other columns that have missing values it is hard to say that samples were taken for these wines for these attributes or maybe something tainted the reading for them.

Review of all variable distributions:

```{r review_distributions,echo=F}

#input_df <- input_df |> dplyr::select(-INDEX)

numeric_train <- input_df[,sapply(input_df, is.numeric)]
par(mfrow=c(4,4))
par(mai=c(.3,.3,.3,.3))
variables <- names(numeric_train)
for (i in 1:(length(variables))) {
  hist(numeric_train[[variables[i]]], main = variables[i], col = "lightblue")
}

```

The vast majority of predictor variables appear to have a fairly normal approximation although `ResidualSugar`, `Chlorides`,`FreeSulfurDioxide`,`CitricAcid` have some minor skew in their distributions. `AcidIndex` appears to more closely resemble a poisson or maybe exponential distribution and the skew may require further transformations. As discussed previously, `STARS` given it's more discrete values in nature is skewed, but likely should not be treated as a numeric input. `LabelAppeal` does seem to be discrete as well which makes sense given its perhaps a standardized marketing score based on consumer feedback. 
The response variable appears to have a large amount of zero values, which requires some further review:

```{r target_dist, echo=F}

ggplot(input_df, aes(x = TARGET, y = after_stat(density))) +
  geom_histogram(binwidth = 1, fill = 'lightblue', color = 'black', alpha = 0.7, 
                 aes(color = "Histogram")) +
  geom_line(aes(y = dpois(TARGET, mean(TARGET), log = FALSE), color = "Poisson"), 
            linewidth = 1) +
  geom_line(aes(y = dnbinom(x = TARGET, size = 1, prob = 0.2), color = "Negative Binomial"), 
            linewidth = 1) +
  geom_line(aes(y = dnorm(x = TARGET, mean = mean(TARGET), sd = sd(TARGET), log = FALSE), color = "Normal"), linewidth = 1) +
  labs(title = 'Target Histogram and Distribution Overlay',
       x = 'TARGET',
       y = 'Density') +
  scale_color_manual(values = c( "red", "blue", "orange"),
                     labels = c("Negative Binomial","Normal","Poisson")) +
  theme_minimal()

```

One key assumption when using poisson regression models is that the response is expected to mirror a poisson distribution which has been plotted on the graph above. An attempt will be made to use a standard poisson model despite the fact that there are many zero values from this distribution. It may be necessary to incorporate zero-inflated modeling techniques to occur for this pattern in the data to improve the accuracy/performance of our regression models. The negative binomial function seems to be account for the frequency of zero values in the `TARGET` response, but it is not capturing the distribution of the remaining predicted cases very well. The normal distriubtion was also included to compare against the poisson given that at higher $\lambda$ values these two distributions tend to converge and in this case are not that substantially different from one another when evaluating the training dataset.

Reviewing $\lambda$ for the poisson distribution:

```{r pois_lambda,echo=F}
print(glue("The response mean: {mean(input_df$TARGET)} and variance: {var(input_df$TARGET)}"))
```

Another important assumption for poisson models relates to the fact that the $\lambda$ parameter which is a critical input for the poisson distribution expects that the  mean and variance must be equal. In practice this is nearly impossible and although there is some overdispersion for the response it is not bad enough to disqualify using this method. 

Let's analyze the correlation relationship among predictors and the response:

```{r check-multicolin, echo=F,out.width='90%'} 

corrplot(cor(na.omit(input_df)),method="color",diag=FALSE,type="lower",addCoef.col = "black",number.cex=0.60)

```

After excluding the NA values, there are weak correlation values across the board except for `TARGET`,`STARS` and `LabelAppeal`. This is not the most encouraging evidence that the predictor variables provided in the dataset are going to be that predictive.

One hypothesis for the apparent lack of linear relationships between variables and the response may be driven off the substantial number of zero values in the response. A potential guess for missing `TARGET` values might be the fact that many more brands of wine exist that are minimally distributed on a commercial scale.

What is the relationship among the variables when excluding the zero response values?

```{r non_zero_corr,echo=F}
nonzero_df <- na.omit(input_df) |> filter(TARGET!=0) 
corrplot(cor(nonzero_df),method="color",diag=FALSE,type="lower",addCoef.col = "black",number.cex=0.60)
```

There aren't many changes across the correlations after excluding the zero response rows. `LabelAppeal` appears to have a stronger linear relationship with the `TARGET` and `STARS`. Two different acidity (`AcidIndex` and `FixedAcidity`) inputs also have more correlation than before although it is still a fairly weak relationship.


### Data Preparation

As discussed in the exploratory section, the `STARS` variable appears to be something that can be treated as a factor. The number of NA values would likely indicate that it is not a rated wine by the experts and much less likely to have sample cases purchased by distributors. The missing values will be updated to zero to mirror the scale of this discrete predictor.

```{r prep_stars, echo=F}
mod_df <- input_df |> dplyr::select(-INDEX) |> mutate(STARS=as.factor(ifelse(is.na(STARS),0,STARS)))
```

The remaining columns with missing values are far less clear to evaluate and will need a more standardized imputation method:

One more comprehensive method to evaluate if the data is Missing Completely at random (MCAR) is running Little's test although this is unlikely to be the case for most real world datasets.

```{r missing_completely_at_random}
littles_test <- input_df |>
    mcar_test()
knitr::kable(littles_test, format = "simple")

```

The zero in the p-value indicates that the missing values across these columns are not in fact MCAR.

Let's review if the NA values appear in similar observations:

```{r missing_values_map,echo=F}
na_cols <- c("pH", "ResidualSugar", "Chlorides", "FreeSulfurDioxide",'Alcohol','TotalSulfurDioxide','Sulphates')
na_col_review <- mod_df |>
    dplyr::select(all_of(na_cols)) |>
    missing_plot()
na_col_review


```

The plot above highlights via a heat map all of the individuals rows where the independent predictors have NA values (shaded in light blue). It's a bit challenging to discern where observations have more than one null value, but there is definitely some overlap across records.

```{r row_index,echo=F}
na_freq <- mod_df |> dplyr::select(all_of(na_cols)) |> mutate(row_index=1:nrow(mod_df)) |> pivot_longer(cols=na_cols,names_to='header') |> filter(is.na(value)) |> group_by(row_index) |> summarise(num_na=n())

ggplot(na_freq,aes(x=num_na)) +
    geom_bar() +
    labs(x='Number of NA columns',y='Number of Rows',title='Distribution of NA Values across columns')

print(glue("Only {pull(na_freq |> filter(num_na>1)|>summarise(total=n()))} rows have more than 1 column with NA, which is only {format(round(pull(na_freq |> filter(num_na>1)|>summarise(total=n())/nrow(na_freq))*100,2),nsmall=2)}%"))

```

It does not appear that many observations have more than 1 NA value and the heatmap was a little bit less clear in helping us identify overlapping columns.


```{r train_test_split,echo=F}
set.seed(19)
rows <- sample(nrow(mod_df))
sample <- sample(c(TRUE, FALSE), nrow(mod_df), replace=TRUE,
                 prob=c(0.7,0.3))
train_df <- mod_df[sample, ]
test_df <- mod_df[!sample, ]

```


```{r imputation}

df_names <- colnames(train_df)
non_na_cols <- df_names[!df_names %in% na_cols]

if (file.exists("imputed_wine_test_df.csv") & file.exists("imputed_wine_train_df.csv")){
    print('yes')
    train_df_imputed_input <- read.csv("imputed_wine_train_df.csv", na.strings = "")
    test_df_imputed_input <- read.csv("imputed_wine_test_df.csv", na.strings = "")
}else{
        #Train Data Imputation First
    init = mice(train_df, maxit=0) 
    meth = init$method
    predM = init$predictorMatrix
    
    
    meth[non_na_cols] = ""
    
    meth[c("pH")] = "pmm"
    meth[c("ResidualSugar")] = "pmm"
    meth[c("Chlorides")] = "pmm"
    meth[c("FreeSulfurDioxide")] = "pmm"
    meth[c("Alcohol")] = "pmm"
    meth[c("TotalSulfurDioxide")] = "pmm"
    meth[c("Sulphates")] = "pmm"

    imputed_train = mice(train_df, method=meth, predictorMatrix=predM, m=5,
                   printFlag = FALSE)
    train_df_imputed <- complete(imputed_train)
    write.csv(train_df_imputed, "imputed_wine_train_df.csv", row.names = FALSE,
              fileEncoding = "UTF-8")
    
    #Repeat for test_df
    init = mice(test_df, maxit=0) 
    meth = init$method
    predM = init$predictorMatrix
    meth[non_na_cols] = ""
    meth[c("pH")] = "pmm"
    meth[c("ResidualSugar")] = "pmm"
    meth[c("Chlorides")] = "pmm"
    meth[c("FreeSulfurDioxide")] = "pmm"
    meth[c("Alcohol")] = "pmm"
    meth[c("TotalSulfurDioxide")] = "pmm"
    meth[c("Sulphates")] = "pmm"
    imputed_test = mice(test_df, method=meth, predictorMatrix=predM, m=5,
                   printFlag = FALSE)
    imputed_test_df <- complete(imputed_test)
    write.csv(imputed_test_df, "imputed_wine_test_df.csv", row.names = FALSE,
              fileEncoding = "UTF-8")

}

```


```{r ignore,echo=F}
#train_df_imputed_input <- read.csv("/Users/JoshForster/Desktop/Masters_Data_Sci/Data621/imputed_wine_train_df.csv", na.strings = "")
#test_df_imputed_input <- read.csv("/Users/JoshForster/Desktop/Masters_Data_Sci/Data621/imputed_wine_test_df.csv", na.strings = "")

```

Let's confirm that the imputation filled all the null values:

```{r impute_check,echo=F}
x <- sapply(train_df_imputed_input, function(x) sum(is.na(x)))
y <- sapply(test_df_imputed_input, function(x) sum(is.na(x)))
sum(x, y) == 0
```

As expected the MICE imputed estimate values for all the missing fields and we can now include all observations within the model.


Let's review the distributions of the train and test imputed datasets to confirm they remain similar to the initial unmodified dataset:

```{r review_imputedistrib, warning = FALSE, message = FALSE}
#need to fix for spacing
sub_vars <- c('Chlorides','pH','Sulphates')
other_nas <- c('ResidualSugar','FreeSulfurDioxide','Alcohol','TotalSulfurDioxide')
impute_train_input <- train_df_imputed_input |>
    dplyr::select(all_of(sub_vars)) |>
    mutate(Set = "Train")
impute_test_input <- test_df_imputed_input |>
    dplyr::select(all_of(sub_vars)) |>
    mutate(Set = "Test")
impute_both <- impute_train_input |>
    bind_rows(impute_test_input)
impute_pivot <- impute_both |>
    pivot_longer(!Set, names_to = "Variable", values_to = "Value")
impute_plot <- impute_pivot |>
    ggplot(aes(x = Value)) +
    geom_density(fill = "lightblue", color = "black") +
    labs(y = "Density") +
    facet_grid(rows = vars(Set), cols = vars(Variable),
               switch = "y", scales = "free_x")
impute_plot

```

These columns were separated as they had higher density points distorting the remainder of predictors that had NA values in the density plots. Across train and test sets it still appears to be similarly distributed and not substantially different from the initial input data.

```{r review_other_imputedistrib, warning = FALSE, message = FALSE}


impute_train_input2 <- train_df_imputed_input |>
    dplyr::select(all_of(other_nas)) |>
    mutate(Set = "Train")
impute_test_input2 <- test_df_imputed_input |>
    dplyr::select(all_of(other_nas)) |>
    mutate(Set = "Test")
impute_both2 <- impute_train_input2 |>
    bind_rows(impute_test_input2)
impute_pivot2 <- impute_both2 |>
    pivot_longer(!Set, names_to = "Variable", values_to = "Value")
impute_plot2 <- impute_pivot2 |>
    ggplot(aes(x = Value)) +
    geom_density(fill = "lightblue", color = "black") +
    labs(y = "Density") +
    facet_grid(rows = vars(Set), cols = vars(Variable),
               switch = "y", scales = "free_x")
impute_plot2

```

The remaining columns that had imputations also mirror one another in the train and test sets as well as the initial distributions. We can proceed with additional review of other variables that need transformations.

For the most part the predictors themselves seemed approximately normal, but there is one additional variable, `AcidIndex` that remains skewed that may require further transformation.

```{r box_cox_transform,echo=F}
acid_bc <- boxcox(lm(train_df_imputed_input$AcidIndex ~ 1))
acid_lambda <- acid_bc$x[which.max(acid_bc$y)]

```

The maximum likelihood estimation for this variable was -1.15 although we are bit skeptical that this transformation is necessary or will approximately normalize the distribution of this column.


Let's see what the distribution of `AcidIndex` looks like with the reciprocal transformation $\frac{1}{x}$ :

```{r plot_acidbc,echo=F}
train_df_imputed_input |> mutate(bc_acidindex=(AcidIndex ^ round(acid_lambda) - 1)/round(acid_lambda)) %>%
    ggplot(aes(x=bc_acidindex)) +
    geom_histogram() +
    labs(title='Testing Proposed Box Cox (-1) Transformation of AcidIndex',x='AcidIndex Transformed')
```

As expected it does not seem warranted to transform this variable after reviewing the histogram of the transformed predictor given that the data is still fairly sparse although it does have a more bell shape than before it doesn't seem to improve the distribution enough to warrant implementation.


Before proceeding further let's double check the correlations of the target and predictor variables to see how imputation impacted the linear relationships across the columns:

```{r impute_corrplot,echo=F}
corrplot(cor(train_df_imputed_input),method="color",diag=FALSE,type="lower",addCoef.col = "black",number.cex=0.60)
```

By modifying the `STARS` into a factor we increased its linear relationship with the target and now that all of the NA observations are included it has dramatically increased the strength with the response. The `LabelAppeal` seems to have decreased quite a bit although it's relationship is far stronger than most of the other potential predictors. Perhaps it's worth considering modifying this variable into a factor as well to see if that impacts the performance. Depending on model performance this might be something worth exploring. `AcidIndex` has the strongest negative relationship among the predictors based on the imputed observations has a somewhat strengthened relationship with `FixedAcidity`. Semantically the two names of these variables appear as though they should have some relationship, but it is not that strong of a correlation.


### Modeling

#### Poisson Count Regression Models

#### Model POIS:1 - Full Model with All Missing Values Imputed - Reduced via Stepwise AIC Model Selection

We create Model 1: Poisson with all missing values imputed and we perform stepwise model selection to select the model with the smallest AIC value using the `stepAIC()` function from the `MASS` package. 

A summary of Model 1: Poisson below:

```{r pois1,echo=F}
mod1_pois <- glm(TARGET ~ ., family = 'poisson',
                   data = train_df_imputed_input)
mod1_pois <- stepAIC(mod1_pois, trace = 0)
summary(mod1_pois)

```

The AIC of Model BLR:1 is 32796 which appears to be very high overall and may indicate other problems. We will assess the summary after building a zero inflated equivalent as a comparison.

Residual Deviance: Chi-Squared Test:


```{r}
with(mod1_pois, cbind(res.deviance = deviance, df = df.residual,
  p = pchisq(deviance, df.residual, lower.tail=FALSE)))
```

This test measures the goodness of fit of the poisson model and an ideal case is if when the test returns a non-significant p-value. It is designed to compare the expected counts (response) returned from the model to the observed values and if the model did as expected there would not be a statistically significant difference between the two values. Therefore, this would indicate that the data dooes not fit the model well and given that all the available predictor variables were initially included that we violated the linear distribution of the Poisson and there was more dispersion than allowed for the Poisson.

```{r}
E.y. <- predict(mod1_pois,type="response")
plot(log(E.y.+1),log(train_df_imputed_input$TARGET+1),xlim=c(0,8))
```


```{r}
m1null <- update(mod1_pois, . ~ 1)

pchisq(2 * (logLik(mod1_pois) - logLik(m1null)), df = 8972, lower.tail = FALSE)
```

```{r}
anova(mod1_pois, test="Chisq")
```

```{r}
faraway::halfnorm(residuals(mod1_pois))
```

```{r}
res <- residuals(mod1_pois, type="deviance")
plot(log(predict(mod1_pois)), res)
abline(h=0, lty=2)
qqnorm(res)
qqline(res)
```


Let's review the diagnostics of the model:

```{r}
car::influencePlot(mod1_pois)
```

It appears that point 1130 is a leverage point; however, it doesn't cross the -2 boundary that would classify it as a true outlier for the model

We check for over/underdispersion of the model to see if there are violations of assumptions

```{r pois_dispersion,echo=F}
## Check for over/underdispersion in the model
E2 <- resid(mod1_pois, type = "pearson")
N  <- nrow(train_df_imputed_input)
p  <- length(coef(mod1_pois))   
sum(E2^2) / (N - p)
```


It appears there is a decent amount of under-dispersion in this model, which may limit our abilities to reliably use Poisson Regression for this count data response.

Alternatively, it does appear that the frequency of zero values does not follow a standard Poisson distribution

```{r zero_inflated_poisson,echo=F}
mod2_zip <- pscl::zeroinfl(formula=TARGET ~ STARS + LabelAppeal + Alcohol + AcidIndex| STARS + LabelAppeal,data=train_df_imputed_input,dist='poisson')


summary(mod2_zip)
```

```{r}
m2null <- update(mod2_zip, . ~ 1)

pchisq(2 * (logLik(mod2_zip) - logLik(m2null)), df = 4, lower.tail = FALSE)
```

A statistically significant result in the slightly modified Chi-Squared Test would seem to indicate that this is a much better fitting model

```{r aic_pois,echo=F}
AIC(mod1_pois,mod2_zip)
```

While the AIC values are not vastly different it does appear obvious that the zero inflated Poisson regression model is preferable to the first poisson regression model. This likely has to do with the strong concentration of zero values that exist in the response variable that is accounted for as a separate logistic model in the zero inflated method. Let's review both of the models to compare the similarities and differences from the summaries and model coefficients despite some obvious limitations identified with the the first model using poisson regression.





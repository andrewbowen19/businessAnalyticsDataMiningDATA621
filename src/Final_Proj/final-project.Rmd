---
title             : "An Analysis of the Department of Education Quality Survey and Its Efficacy"
shorttitle        : "DATA621 Final Project"

author: 
  - name          : "Andrew Bowen"
    affiliation   : "1"
    corresponding : no    # Define only one corresponding author
    email         : "andrew.bowen08@spsmail.cuny.edu"
  - name          : "Glen Dale Davis"
    affiliation   : "1"
  - name: "Josh Forster"
    affiliation: "1"
  - name: "Shoshana Farber"
    affiliation: "1"
  - name: "Charles Ugiagbe"
    affiliation: "1"

affiliation:
  - id            : "1"
    institution   : "City University of New York"


abstract: | 
  We present a study on the effectiveness of New York City School Quality Snapshot in predicting the 4-year college persistence rate for New York City high schools. These surveys are used in the deciding of educational policy across the United States. While input from educators and families can be invaluable, it often can not reflect the underlying factors that most influence academic performance. We also build a predictive model based on proxy socioeconomic factors (presence in temporary housing, economic need) that can also be used to predict the average college persistence of a high school in NYC. Our model based on socioeconomic indicators outperforms a model based solely on results of the school quality snapshot, which includes survey responses from students, parents, and educators within NYC public schools. This increased performance comes across several model diagnostic statistics, including root mean-squared error, Akaike and Bayesian Information Criteria, and adjusted R-squared. Additionally, a weighted least-squares model is created on the same set of proxy variables to compare modeling techniques. This model outperforms the ratings-based linear model, but introduces additional complexity for less explainability when compared with the direct proxy variable model. Lastly, we train a radial-basis support vector machine (SVM-RB) model and a Lasso regression model to identify relationships in the underlying data not identified by our linear models. From the features identified in our SVM we run a multiple linear regression model (MLR) to quantify these backward-selected variables' contributions as well, to identify the strongest predictors of a high school;s college persistence rate.

  
  
keywords          : "Educational Outcomes, School Quality, Education"

bibliography      : "r-references.bib"

floatsintext      : no
linenumbers       : no
draft             : no
mask              : no
figurelist        : no
tablelist         : no
footnotelist      : no
figsintext        : yes
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup-knitr, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r setup, include = FALSE, echo=FALSE, message=FALSE}
library(tidyverse)
library(gridExtra)
library(glue)
library(mice)
library(corrplot)
library(caret)
library(modelr)
library("papaja")
library(DataExplorer)
library(cowplot)
library(car)
library(AICcmodavg)
library(rminer)
library(elasticnet)
r_refs("r-references.bib")
```

# Introduction
The NYC School Survey seeks to collect data to provide an overview of New York City (NYC) Schools. First conducted in 2005, the survey gathers demographic and achievement data for NYC Public Schools and provides a standardized rating of various elements of school quality. 

The survey has changed over the years. These changes have come from the recommendations of public policy analysts seeking to more accurately define the quality of schools *@redesign-school-survey*. The 2020-21 academic year report provides a robust dataset of school-level observations of academic and socioeconomic data.

**Research Question:** Our analysis aims to determine whether NYC School Quality Survey ratings accurately reflect educational outcomes or if these outcomes could be better predicted by proxy variables related to the student body.

The primary measure of success we aim to predict is the 4-year college persistence rate for NYC high schools. This measure is defined as the percentage of students who graduate from a high school and eventually complete a 4-year college program. Identifying the key indicators of a school's ability to successfully prepare students for college could benefit the NYC Department of Education (DOE) and NYC Public Schools in several ways:

1. It would provide insights to the NYC DOE and NYC Public Schools which would enable them to tailor instructional approaches and develop targeted curricula that specifically address college preparedness. 
2. It would allow for strategic allocation of resources to address identified areas that significantly impact college readiness, ensuring that resources are utilized efficiently to increase the percentage of college-ready students across NYC Public Schools. 

It is well-established that attending 4-year institutions significantly enhances career potential earnings. Ensuring that high school students are adequately prepared for their college careers not only benefits their immediate educational success but also contributes to their long-term success in life.

# Literature Review
One of the main predictors of academic performance is a student's socioeconomic background. According to the National Center for Education Statistics (NCES), students from low-income families are nearly four times more likely to drop out of high school than students from wealthy families *@NCES-Dropout-Rates*. 

Several prior studies have made attempts to use more sophisticated modeling techniques, different data sources, and different predictor variables to predict educational outcomes similar to what we're trying to predict. In one such study, *@BERNACKI2020103999* based their modeling on trying to predict educational achievement based on student digital behavior, rather than the social factors we intend to explore. The model in this study reached an accuracy of 75%, and was able to flag early interventions. This modeling technique attempts to predict a slightly different metric of student success than our modeling will, and the training data and predictor variables differ as well.

Similarly, *@MUSSO202000104* attempted to train an artificial neural network (ANN) to identify relationships between variables and educational performance data. They modeled educational performance of Vietnamese students in grade five and included individual characteristics as well as information related to daily routines in their training data. This method uses a more sophisticated model, and resulted in an impressive prediction accuracy of $95-100%$. However, as their training data comes from a different country with a different educational system and methods, it may not be prudent to compare the model's results to those of our model or of any other US-centric study.

In another study, *@yagci-educational-2022* predicted final grade exams for Turkish students through machine learning models, using prior exam scores as their input variables. While this provides a valuable metric for academic performance, concerns arise regarding the direct correlation between good exam grades and later career success *@Grades-and-Careers*. However, a parent study found a correlation of up to 0.3 between academic grades and later job performance *@roth_meta-analyzing_1996*, so it may be worthwhile to consider this metric as a measure to predict later success in life. Further analysis would have to be conducted in this respect. 

Measuring which predictors impact educational outcomes and how much is a difficult task. There are generally many confounding variables related to the student body being observed, and causal relationships can be difficult if not impossible to establish. 

# Data Sourcing
The dataset used in this study is published in the [NYC School Quality Report for the Academic Year 2020 - 2021](https://data.cityofnewyork.us/Education/2020-2021-School-Quality-Reports-High-School/26je-vkp6). It consists of data from 487 NYC Public Schools, and there are 391 variable columns. The observations are all school-level, indexed by each school's *District Borough Number* (DBN). 

In addition to the school quality ratings based on survey responses, average and raw academic performance data are included as well. There are also socioeconomic variables, such as a school's percentage of students in temporary housing services.

```{r data-read-in}
# Read in our dataset from GitHub
# https://www.opendatanetwork.com/dataset/data.cityofnewyork.us/bm9v-cvch
df <- read.csv("https://data.cityofnewyork.us/api/views/26je-vkp6/rows.csv?date=20231108")
```
# Methodology
Our primary interest is finding proxy variables within the data that can better serve as predictors of 4-year college persistence rates at a given NYC high school than the school survey ratings collected by the quality review. Toward this end, we will need to first construct a baseline model that predicts a school's college persistence rate.

We will attempt to use three variables as a proxy for the school's survey rating in predicting college persistence:

- `temp_housing_pct`: the percentage of students living in temporary housing
- `eni_hs_pct_912`: [Economic Need Index](https://data.cccnewyork.org/data/bar/1371/student-economic-need-index#1371/a/1/1622/127): a measure of the percentage of students facing economic hardship at a school^[**noauthor_student_2021 (fix, not in references)** Economic hardship in this context is based on three criteria: whether the student is 1) eligible for public assistance from the NYC Human Resources Administration (HRA); 2) lived in temporary housing in the past four years; 3) is in high school, has a home language other than English, and entered the NYC DOE for the first time within the last four years.]
- `val_chronic_absent_hs_all`: the percentage of students who are chronically absent^[Chronic absenteeism is defined by the NYC DOE as "students who are absent 10 percent or more of the total days."]

```{r, echo=FALSE, warning=FALSE}
label_cols <- c("dbn", "school_name", "school_type")
# Convert needed columns to numeric typing
df <- cbind(df[, label_cols], as.data.frame(lapply(df[,!names(df) %in% label_cols], as.numeric)))

df$college_rate <- df$val_persist3_4yr_all
df <- df |>
    select(-val_persist3_4yr_all)
df$economic_need <- df$eni_hs_pct_912
df <- df |>
    select(-eni_hs_pct_912)
```

We begin by taking a look at a summary of the dataset's completeness.

```{r data2}
remove <- c("discrete_columns", "continuous_columns",
            "total_observations", "memory_usage")
completeness <- introduce(df) |>
    select(-all_of(remove))
apa_table(t(completeness), caption = "Completeness Summary", placement = "H")

```
There are 12 columns that are completely devoid of data, so we identify and remove those.

```{r }
find_all_na_cols <- function(dframe){
    col_sums_na <- colSums(is.na(dframe))
    all_na_cols <- names(col_sums_na[col_sums_na == nrow(dframe)])
    all_na_cols
}
all_na_cols <- find_all_na_cols(df)
df <- df |>
    select(-all_of(all_na_cols))
all_na_cols <- as.data.frame(all_na_cols)
colnames(all_na_cols) <- c("All NA Columns")
apa_table(all_na_cols, placement = "H")

```
We create a 20% holdout set of data to be used later on in order to evaluate the efficacy of our model's predictive capability. The remaining 80% of the data is to be used for model training and exploratory data analysis (EDA).

```{r train-test-split}
set.seed(42)

# Adding a 20% holdout of our input data for model evaluation later
train <- subset(df[sample(1:nrow(df)), ]) %>% sample_frac(0.8)
train_svm <- train |> select(-all_of(c("dbn", "school_name", "school_type")))
na_count <-sapply(train_svm, function(y) sum(length(which(is.na(y)))))
na_count <- data.frame(na_count)
na_count <- na_count |>
    filter(na_count == 0)
incl <- rownames(na_count)
train_svm <- train_svm |>
    select(all_of(c("college_rate", incl))) |>
    drop_na(college_rate)
test  <- dplyr::anti_join(df, train, by = 'dbn')
test_svm <- test |> select(-all_of(c("dbn", "school_name", "school_type")))
test_svm <- test_svm |>
    select(all_of(c("college_rate", incl))) |>
    drop_na(college_rate)

```
For ease of single-node computation, we reduce our primary datasets to our primary variables of interest. Notably, these are the survey ratings, enrollment levels, and our preferred proxy variables for each school. We retain secondary datasets, which contain all variables for which there are no missing values, for later use when developing certain models. 

```{r }
cols <- c("survey_pp_CT", "survey_pp_RI",
          "survey_pp_ES", "survey_pp_SE",
          "survey_pp_SF", "survey_pp_TR",
          "temp_housing_pct", "economic_need",
          "college_rate", "enrollment",
          "val_chronic_absent_hs_all")
train_data <- train[, cols]
```
We take a look at whether the reduced training dataset contains any missing values and what the spread is.

```{r data3, include = FALSE}
p1 <- plot_missing(train_data, missing_only = FALSE,
                   ggtheme = theme_classic(), title = "Missing Values")

```

```{r data4, fig.pos='H', out.width="\\textwidth", warning=FALSE, message=FALSE}
# Plot missing value percentages by cols of interest
p1 <- p1 + 
    scale_fill_brewer(palette = "Paired")
p1
```
The variable with the most missing data is `college_rate`. Some schools are also missing some survey ratings, and a very small percentage of schools are missing chronic absenteeism values.

We impute both our training and evaluation datasets. Given we are dealing with continuous numeric (and not categorical variables), we use the *Predictive Mean Matching* imputation method native to the R `mice` package.

```{r impute-train, echo=FALSE, message=FALSE}
imp <- mice(train_data, method="pmm", seed=42, printFlag = FALSE)
train <- complete(imp)
```

```{r impute-test, echo=FALSE, message=FALSE}
test_data <- test[, cols]
imp <- mice(test_data, method="pmm", seed=42, printFlag = FALSE)
test <- complete(imp)
```
To check underlying modeling assumptions, we plot distributions and relationships of different variables. First, we plot the distribution of college persistence rates among NYC high schools to check for normality.

```{r fig.pos='H', out.width="\\textwidth"}
# Plot target variable distribution
ggplot(train, aes(x=college_rate)) + 
    geom_density() + 
    labs(x="4-Year College Persistence Rate",
         y="Density of NYC High Schools",
         title="Average 4-Year College Persistence Rates: NYC High Schools 2020-2021",
         caption="The average NYC high school sees ~50% of students go on to have 4-year college persistence.")

theme_set(theme_apa())
```
We see a relatively normal distribution of college persistence rates. In the case of NYC high schools, the peak is at around 50%. This is inline with national averages released by the *@CensusBureau_CollegeRates_2023*.

The below plot shows the raw correlation between each variable in our pared down dataset (*Collaborative Teaching*, *Trust*, etc) and the response variable of interest: *4-Year College Persistence Rate*.

```{r fig.pos='H', out.width="\\textwidth"}
# Renaming training dataframe for correlation plot
train_renamed <- train %>%
  rename("Collaborative Teaching"=survey_pp_CT,
         "Rigorous Instruction"=survey_pp_RI,
         "Supportive Env"=survey_pp_SE,
         "Effective Leadership"=survey_pp_ES,
         "Family-Community Ties"=survey_pp_SF,
         "Trust"=survey_pp_TR,
         "Temporary Housing Pct"=temp_housing_pct,
         "Economic Need"=economic_need,
         "College Persistence"=college_rate,
         "Enrollment"=enrollment,
         "Chronic Absenteeism"=val_chronic_absent_hs_all)

# Create correlation plot between vars of interest
corMatrix <- cor(train_renamed)
corrplot(corMatrix, method="color", type="lower", tl.col="black", addCoef.col = "black", tl.cex = 0.7, number.cex = 0.6)

```
From our correlation plot above, we can see strong negative relationships between two of our proxy variables of interest (*Temporary Housing Rate* and *Economic Need Index*) and our target variable: *College Persistence Rate*. There is also a negative relationship between *Chronic Absenteeism* and *College Persistence Rate*, but to a lesser degree. This gives signal that constructing models based on these variables could give good insight into the factors that most influence college persistence.

*Enrollment* has only a slightly positive relationship with *College Persistence Rate*. We expected school size might be important when modeling, but that does not appear to be likely. 

We also see that the survey ratings are all at least somewhat positively correlated with one another, and the only survey rating that appears to have a relationship with *College Persistence Rate* is *Rigorous Instruction*. That relationship is only slightly positive. This signals that constructing a model based on one or more of the survey ratings might not give as much insight into college persistence as the proxy variables could. 

Now we can plot the distributions of our proxy variables of interest.

```{r temp-housing-rates, fig.pos='H', out.width="\\textwidth"}
# Plot temp housing rates
pa <- ggplot(train, aes(x=temp_housing_pct)) +
    geom_density() +
    labs(x="% Students Temp Housing", y="Density")
# Plot economic need index
pb <- ggplot(train, aes(x=economic_need)) +
    geom_density() +
    labs(x="Economic Need Index", y="Density")
# Plot enrollment
pc <- ggplot(train, aes(x=enrollment)) +
    geom_density() +
    labs(x="Enrollment", y="Density")
# Plot chronic absenteeism
pd <- ggplot(train, aes(x=val_chronic_absent_hs_all)) +
    geom_density() +
    labs(x="Chronic Absenteeism", y="Density")
p <- plot_grid(pa, pb, pc, pd, nrow = 2, ncol = 2, align = "hv", axis = "t")
p

```
We see the distribution of *Temporary Housing Rate* is right-skewed. We also see the distribution of *Economic Need Index* is left-skewed. The closer the index is to 1, the more economic hardship students at that school face, so schools with high rates of students facing economic hardship are more prevalent than schools with low rates. These variables are both candidates for transformation due to their skew. Our model will not likely feature *Enrollment*, as observations are so concentrated at the low end, and we already noted it is not as correlated with our target variable as the other proxies we're considering. *Chronic Absenteeism* is closer to a normal distribution than the other variables, but it is still slightly right-skewed, so there are more schools in this dataset with pretty low rates and fewer schools with pretty high rates. 

We check an assumption of linearity between our proxy predictors and our response variable by producing scatter plots of the response variable versus each of the proxy predictors. 

```{r fig.pos='H', out.width="\\textwidth"}
# Plot temp housing percentage vs college persistence rate
pa <- ggplot(train, aes(x=temp_housing_pct, y=college_rate)) +
  geom_point() +
  labs(x="% Students in Temp Housing",
       y="College Persist")
# Plot ENI vs college persistence rate
pb <- ggplot(train, aes(x=economic_need, y=college_rate)) +
  geom_point() +
  labs(x="Economic Need Index",
       y="College Persist")
pc <- ggplot(train, aes(x=enrollment, y=college_rate)) +
  geom_point() +
  labs(x="Enrollment",
       y="College Persist")
pd <- ggplot(train, aes(x=val_chronic_absent_hs_all, y=college_rate)) +
  geom_point() +
  labs(x="Chronic Absenteeism",
       y="College Persist")
p <- plot_grid(pa, pb, pc, pd, nrow = 2, ncol = 2, align = "hv", axis = "t")
p

```
We see a generally negative linear relationship between the response variable and rates of students in temporary housing. As that rate increases, college persistence tends to decrease. However, that relationship does **not** appear to hold for schools with higher rates of students in temporary housing. So the relationship cannot be completely captured by a linear trend. 

We also see a non-linear relationship between the response variable and the economic need index. 

Schools with lower enrollment levels have a wider range of college persistence rates than schools with higher enrollment levels.

Only one school where chronic absenteeism is greater than or equal to 50 percent achieves college persistence levels above 80 percent. However, college persistence varies widely at most chronic absenteeism levels. 

# Modeling
For evaluation purposes, we create a linear model based on the survey ratings present per school in our data. We fit this multiple least-squares model to predict the college persistence rate of a given high school. The model summary is printed below:

```{r base-rating-model}
base_formula <- college_rate ~ survey_pp_CT + survey_pp_RI + survey_pp_SE + survey_pp_ES + survey_pp_SF + survey_pp_TR
rating_model <- lm(base_formula,
                   train)
summary(rating_model)

```
We find our base model for the school survey ratings produces an adjusted R-squared of $R^2_{adj} = 0.24$. This is lower than the predictive model in *@roth_meta-analyzing_1996* produces. The two survey ratings that appear to be statistically significant to the model are *Rigorous Instruction*, which we expected based on our correlation analysis, and *Supportive Environment*, which we did not expect. We reduce the model via backward selection, and *Effective Leadership* becomes statistically significant as well. We reprint a summary below:

```{r base-rating-model-backward-selection}
rating_model <- update(rating_model, ~ . - survey_pp_CT - survey_pp_SF - survey_pp_TR)
summary(rating_model)

```
The adjusted R-squared is the same due to rounding. We check for suspected multicollinearity within this model:

```{r base-rating-model-vif-check}
vif_df <- as.data.frame(vif(rating_model))
colnames(vif_df) <- c("VIF Value")
apa_table(vif_df, caption = "Variance Inflation Factors", placement = "H")
```

Surprisingly, none of the variance inflation factors are greater than five, so there are no multicollinearity issues to address for this model.

Let's look at some diagnostic plots for this model.

```{r base-model-plot, fig.pos='H', out.width="\\textwidth"}
par(mfrow=c(2,2))
par(mai=c(.3,.3,.3,.3))
plot(rating_model)

```

The residuals look relatively normal from the QQ-plot but there seems to be some clustering in the residuals vs fitted plot and we can see a slight curving. 

We then create a basic multiple least squares linear model between the response and our three socioeconomic proxy variables: *Temporary Housing Rate*, *Economic Need Index*, and *Chronic Absenteeism*. We include *Enrollment* as well. The summary statistics of the socieoeconomic model are shown below.

```{r proxy-mlm}
# Create OLS linear model based on our proxy variables: no transforms
proxy_formula <- college_rate ~ temp_housing_pct + economic_need + val_chronic_absent_hs_all + enrollment
proxy_model <- lm(proxy_formula, train)
summary(proxy_model)

```
*Enrollment* is not statistically significant, so we remove it and reprint a summary.

```{r proxy-model-reprint}
proxy_model <- update(proxy_model, ~ . - enrollment)
summary(proxy_model)

```
We find our proxy socioeconomic model produces an adjusted R-squared of $R^2_{adj} = 0.52$. We also check for multicollinearity within this model. Unlike with the base model based on the survey ratings, we do not expect any such issues with this model.

```{r proxy-model-vif-check}
vif_df <- as.data.frame(vif(proxy_model))
colnames(vif_df) <- c("VIF Value")
apa_table(vif_df, caption = "Variance Inflation Factors", placement = "H")

```

None of the variance inflation factors are greater than five, so there are no multicollinearity issues to address for this model.

We produce diagnostic plots for the model below. 

```{r plot-proxy-model, fig.pos='H', out.width="\\textwidth"}
par(mfrow=c(2,2))
par(mai=c(.3,.3,.3,.3))
plot(proxy_model)

```
We see no strong trend in the residuals vs. fitted plot, indicating heteroscedasticity. The residuals also look to be normally distributed. 

We can also test the assumption of normally-distributed residuals via a [Shapiro-Wilk test for normality](https://en.wikipedia.org/wiki/Shapiroâ€“Wilk_test). Here we operate with the null $H_0$ and alternative hypotheses $H_a$:

- $H_0$: the error terms of the socioeconomic proxy model come from a normally-distributed population
- $H_a$:  the error terms of the socioeconomic proxy model come from a population that is **not** normally distributed

```{r shapiro-proxy-model}
# Test proxy model for normality of residuals
shapiro.test(proxy_model$residuals)
```
Running a Shapiro test for normality at a 95% threshold, we receive a p-value of 0.5848, higher than our threshold, so we cannot reject our null hypothesis.

Plotting our proxy model's residuals, we can confirm normality as well visually:
```{r proxy-residual-plot}
hist(proxy_model$residuals, xlab="Proxy Model Residuals")
```


We also fit a weighted-least squares (WLS) model to our proxy variables to account for unqual variances among measurements' residuals. The approach can be used to mitigate the effects of heteroscedastic data when modeling.

```{r weighted-least-squares}
# Calculating weights for WLS
weights <- 1 / lm(abs(proxy_model$residuals) ~ proxy_model$fitted.values)$fitted.values^2

#perform weighted least squares regression
proxy_formula <- proxy_model$call$formula
wls_model <- lm(proxy_formula, data = train, weights=weights)

summary(wls_model)
```


```{r plot-wls-model, fig.pos='H', out.width="\\textwidth"}
par(mfrow=c(2,2))
par(mai=c(.3,.3,.3,.3))
plot(wls_model)
```

From our diagnostic plots of the WLS model above, we see no general pattern in the *Residuals vs Fitted* plot. In addition, our Q-Q plot shows general normality behavior, with some behavior off the trendline near the tails of the distribution. Similar to above, we can run a Shapiro test on the residuals of the WLS model to assess normality of residuals, with the same alternative and null hypotheses as above:


```{r wls-shapiro}
shapiro.test(wls_model$residuals)
```

Again, our p-value of 0.7594 indicates that the underlying distribution of residuals is normally distributed.

In the interest of determining whether there are other important variables in the original data that we have not considered, or whether a nonlinear approach could reveal relationships in the data that our linear models have not captured so far, we train a Support Vector Machine (SVM): Radial Basis (RB) model on centered and scaled data. SVMs effectively find hyperplanes that divide classes within data well, and they are particularly useful for datasets with large numbers of features. (Predictors with `NA` values in the original dataset have been excluded from consideration due to the undesirably large amount of imputation that would have been required, as SVMs can't handle missing data.) Radial Basis is simply one of several kernel functions we could have chosen, and it effectively defines the shape of the classification boundaries the model makes.

```{r }
svmRBTuned <- train(train_svm |> select(all_of(incl)), train_svm$college_rate,
                    method = "svmRadial",
                    preProc = c("center", "scale"),
                    tuneLength = 14,
                    trControl = trainControl(method = "cv"))

```
A summary of the ideal tuning parameters and R-Squared value for the SVM:RB model is below:

```{r }
svm_summ <- c("SVM:RB",
             round(svmRBTuned$bestTune$sigma, 4),
             svmRBTuned$bestTune$C,
             round(svmRBTuned$results |>
                 filter(C == svmRBTuned$bestTune$C) |>
                 select(Rsquared) |> as.numeric(), 4))
svm_summ <- as.data.frame(t(svm_summ))
cols <- c("Model", "sigma", "C", "R-Squared")
colnames(svm_summ) <- cols
apa_table(svm_summ, placement = "H")

```
A summary of the estimated feature importance for the ten most important features in this SVM:RB model is below:

```{r }
y <- train_svm$college_rate
names(y) <- "y"
dat = cbind(train_svm |> select(all_of(incl)), y)
svmRBFit <- fit(y~., data = dat, model = "svm",
               kpar = list(sigma = 0.0244), C = 1)
svmRB.imp <- Importance(svmRBFit, data = dat)
L = list(runs = 1,sen = t(svmRB.imp$imp),
         sresponses = svmRB.imp$sresponses)
sen_vec <- as.numeric(L[["sen"]])
copy <- L
delete <- c()
sort <- sort(sen_vec, decreasing = TRUE)
for (i in 1:length(sen_vec)){
    if (sen_vec[i] > sort[11]){
        next
    }else{
        delete <- append(delete, i)
    }
}
copy[["sen"]] <- t(as.matrix(copy[["sen"]][, -delete]))
copy[["sresponses"]] <- copy[["sresponses"]][-delete]
names <- c()
for (i in 1:length(copy[["sresponses"]])){
    n <- copy[["sresponses"]][[i]][["n"]]
    names <- append(names, n)
}
mgraph(copy, graph = "IMP", leg = names, col = "gray",
       PDF = "")

```
We can see that the SVM:RB model includes two of our proxy variables in its top ten most important features list: `temp_housing_pct` and `economic_need`. However, the single most important feature to the SVM:RB model is `ethnicity_asian_pct`, and the model considers other race and gender percentages important in predicting `college_rate` as well. It is not surprising that the student body race and gender percentages play a role in college persistence rates, and that could be a fruitful alternative avenue of analysis.

We build a multiple linear regression model using the ten most important features identified in the SVM:RB model, reduced via backward selection. 

```{r }
imp_feat_form <- college_rate ~ ethnicity_asian_pct + hs_overage_undercredit_hoi_pct + ethnicity_amerindian_pct + temp_housing_pct + cap_ctt_pct + economic_need + hra_elig_pct + gender_male_pct + gender_female_pct + ELL
imp_feat_model <- lm(imp_feat_form, train_svm)
imp_feat_model <- update(imp_feat_model, . ~ . - ethnicity_amerindian_pct - gender_female_pct - economic_need - temp_housing_pct)
summary(imp_feat_model)

```

The only predictors that remain after backward selection are `ethnicity_asian_pct`, `hs_overage_undercredit_hoi_pct`, `cap_ctt_pct`, `hra_elig_pct`, `gender_male_pct`, and `ELL`. A higher percentage of Asian students has a positive impact on college persistence rates, while having a higher percentage of overage students who are undercredited, having a higher percentage of students recommended for integrated co-teaching, having a higher rate of HRA eligible students, having a higher percentage of male students, and having a higher percentage of students whose language spoken at home is not English all have a negative impact on college persistence rates. (Note that while `economic_need` was not significant to this model after backward selection, some variables that contribute to this composite variable were: `hra_elig_pct` and `ELL`.)

We find our important features model produces an adjusted R-squared of $R^2_{adj} = 0.68$. We also check for multicollinearity within this model.

```{r imp-feat-model-vif-check}
vif_df <- as.data.frame(vif(imp_feat_model))
colnames(vif_df) <- c("VIF Value")
apa_table(vif_df, caption = "Variance Inflation Factors", placement = "H")

```

None of the variance inflation factors are greater than five, so there are no multicollinearity issues to address for this model.

We produce diagnostic plots for the model below. 

```{r plot-imp-feat-model, fig.pos='H', out.width="\\textwidth"}
par(mfrow=c(2,2))
par(mai=c(.3,.3,.3,.3))
plot(imp_feat_model)

```

There are some issues revealed by the diagnostic plots. The residuals vs. fitted values aren't evenly distributed around zero, and the Q-Q residuals deviate from the normal line at the low end. There are no high leverage points, but point 62 comes closest and is one of the problematic points in all the diagnostic plots. We run a Shapiro test on the residuals of the Important Features model to assess normality of residuals, with the same alternative and null hypotheses as above.

```{r imp-feat-shapiro}
shapiro.test(imp_feat_model$residuals)
```

The low p-value here suggests the residuals do not come from a normal distribution. This is not surprising after reviewing the diagnostic plots, and it makes sense that building a multiple linear regression model with variables selected by a nonlinear model could produce non-normal residuals. 

Lastly, we train a lasso model as an alternative means of feature selection. The ridge-regression penalty $\lambda$ is fixed at 0, and we will tune over a number of lasso penalties. These lasso penalties penalize the model according to the sum of the absolute values of weights in it, and as such, we end up with a model where many less important variables are given zero weight. (Predictors with `NA` values in the original dataset have again been excluded from consideration due to the undesirably large amount of imputation that would have been required.)

```{r train_enet,echo=FALSE,message=FALSE}
train_lasso <- train_svm
test_lasso <- test_svm
lassoGrid <- expand.grid(.lambda = c(0),
                         .fraction = seq(.05, 1, length = 20))
ctrl <- trainControl(method = "cv", number = 10)
lassoTune <- train(train_lasso |> select(all_of(incl)),
                   train_lasso$college_rate,
                   method = "enet",
                   tuneGrid = lassoGrid,
                   trControl = ctrl,
                   preProc = c("center", "scale"))

```
A summary of the ideal tuning parameters and R-Squared value for the lasso model is below:

```{r }
lasso_summ <- c("Lasso",
             lassoTune$bestTune$lambda,
             lassoTune$bestTune$fraction,
             round(lassoTune$results |>
                 filter(fraction == lassoTune$bestTune$fraction) |>
                 select(Rsquared) |> as.numeric(), 4))
lasso_summ <- as.data.frame(t(lasso_summ))
cols <- c("Model", "lambda", "lasso penalty", "R-Squared")
colnames(lasso_summ) <- cols
apa_table(lasso_summ, placement = "H")

```
A summary of the estimated feature importance for the ten most important features in this lasso model is below:

```{r }
lasso_imp <- varImp(lassoTune, scale = TRUE)
cols <- c("Predictor", "Importance")
lasso_imp <- lasso_imp$importance |>
    rownames_to_column()
colnames(lasso_imp) <- cols
lasso_imp <- lasso_imp |>
    arrange(desc(Importance)) |>
    top_n(10)
apa_table(lasso_imp, placement = "H")

```

In the lasso model, `temp_housing_pct` and `economic_need` are relatively more important than any race or gender percentage variables, unlike in the SVM:RB model. The most important variable is `hs_overage_undercredit_hoi_pct`. 

Here is a summary of the predicted coefficients in the lasso model, including those given zero weight:

```{r }
lasso_coefs <- as.data.frame(predict.enet(lassoTune$finalModel, s=lassoTune$bestTune[1, "fraction"], type="coef", mode="fraction")$coefficients)
lasso_coefs <- lasso_coefs |>
    rownames_to_column()
cols <- c("Predcitor", "Coef")
colnames(lasso_coefs) <- cols
lasso_coefs <- lasso_coefs |>
    arrange(desc(abs(Coef)))

```

# Experimentation and Results

### Model Evaluation

To evaluate our models' quality and performance, we will separate the models into two groups and measure Predictive R-Squared and RMSE using the holdout test data. The first group of models will be the Survey Ratings, Proxy Variables, and Weighted Least-Squares models, which all use a reduced dataset that only includes features we selected based on our research question. This dataset has `NA` values imputed. The second group of models will be the SVM:RB, Important features, and Lasso models. This group uses the full dataset (excluding predictors that had `NA` values, as well as observations that had `NA` values in the response). It is important to compare them separately because of the differences in their underlying data. We will only measure AIC and BIC for the first group, which are all linear models for which these metrics make sense. 


### Group 1 Models:

```{r pred-rsquared-RMSE}
test_pred1 <- predict(rating_model, test |> select(-college_rate))
test_rsq1 <- as.numeric(R2(test_pred1, test$college_rate, form = "traditional"))
test_rmse1 <- as.numeric(RMSE(test_pred1, test$college_rate))
row1 <- cbind("Survey Ratings",
              as.character(round(test_rsq1, 4)),
              as.character(round(test_rmse1, 4)))
test_pred2 <- predict(proxy_model, test |> select(-college_rate))
test_rsq2 <- as.numeric(R2(test_pred2, test$college_rate, form = "traditional"))
test_rmse2 <- as.numeric(RMSE(test_pred2, test$college_rate))
row2 <- cbind("Proxy Variables",
              as.character(round(test_rsq2, 4)),
              as.character(round(test_rmse2, 4)))
test_pred3 <- predict(wls_model, test |> select(-college_rate))
test_rsq3 <- as.numeric(R2(test_pred3, test$college_rate, form = "traditional"))
test_rmse3 <- as.numeric(RMSE(test_pred3, test$college_rate))
row3 <- cbind("Weighted Least-Squares",
              as.character(round(test_rsq3, 4)),
              as.character(round(test_rmse3, 4)))
tbl <- as.data.frame(rbind(row1, row2, row3))
cols <- c("Model", "Predictive R-Squared", "RMSE")
colnames(tbl) <- cols
tbl <- tbl |>
    mutate(Group = "Group 1") |>
    select(Group, everything())
apa_table(tbl, placement = "H")

```

In Group 1, the Proxy Variables model has the highest Predictive R-Squared and the lowest RMSE. 

We can also use the Akaike and Bayesian Information Criterion for evaluating the complexity of the Group 1 models.


```{r aic-bic, warning=FALSE}
model_names <- c("Survey Ratings", "Proxy Variables", "Weighted Least-Squares")
model_list <- list(rating_model, proxy_model, wls_model)


# Print AIC results
apa_table(aictab(model_list, modnames=model_names), placement="H", caption="Akaike's Information criterion for model's within Group 1. Respectively, this includes our naive ratings model, our proxy variable model, and our WLS model")

# Print BIC for each model
apa_table(bictab(model_list, modnames=model_names), placement="H", caption="Bayes Information criterion for model's within Group 1. Respectively, this includes our naive ratings model, our proxy variable model, and our WLS model")
```
From our tables above, we can see smaller values of corrected AIC (which accounts for smaller sample sizes) from the proxy and WLS models than our base survey rating model. This implies better predictive performance for our WLS and proxy-variable models.

### Group 2 Models:

```{r }
test_pred1 <- predict(svmRBTuned, test_svm |> select(-college_rate))
test_rsq1 <- as.numeric(R2(test_pred1, test_svm$college_rate,
                           form = "traditional"))
test_rmse1 <- as.numeric(RMSE(test_pred1, test_svm$college_rate))
row1 <- cbind("SVM:RB",
              as.character(round(test_rsq1, 4)),
              as.character(round(test_rmse1, 4)))
test_pred2 <- predict(imp_feat_model, test_svm |> select(-college_rate))
test_rsq2 <- as.numeric(R2(test_pred2, test_svm$college_rate,
                           form = "traditional"))
test_rmse2 <- as.numeric(RMSE(test_pred2, test_svm$college_rate))
row2 <- cbind("Important Features (MLR)",
              as.character(round(test_rsq2, 4)),
              as.character(round(test_rmse2, 4)))
test_pred3 <- predict(lassoTune, test_lasso |> select(-college_rate))
test_rsq3 <- as.numeric(R2(test_pred3, test_lasso$college_rate,
                           form = "traditional"))
test_rmse3 <- as.numeric(RMSE(test_pred3, test_lasso$college_rate))
row3 <- cbind("Lasso",
              as.character(round(test_rsq3, 4)),
              as.character(round(test_rmse3, 4)))
tbl <- as.data.frame(rbind(row1, row2, row3))
cols <- c("Model", "Predictive R-Squared", "RMSE")
colnames(tbl) <- cols
tbl <- tbl |>
    mutate(Group = "Group 2") |>
    select(Group, everything())
apa_table(tbl, placement = "H")

```
In Group 2, the SVM:RB model has the highest Predictive R-Squared and the lowest RMSE, but it's a very close match between this model and the Lasso model.

# Conclusion
Overall, our model to predict a high school's college persistence rate based on socioeconomic proxy variables outperformed the NYC Schools Open Survey Quality ratings of schools. This is not to say that school ratings based on teacher, student, and parent responses are not valuable inputs. However, they should not be the sole basis upon which educational policy decisions are made, considering the collective socioeconomic factors that most influence a school's performance.

Some limitations of our approach would come from conflation between socioeconomic factors, as well as lacking a more robust imputation method. While our proxy variable model predicts college persistence better than one based off survey ratings, there could be error via omission of unseen variables that are collinear with these inputs. This stems from the availability of data in our source data. We used a [*predictive mean matching* imputation method](https://stefvanbuuren.name/fimd/sec-pmm.html), native to the `mice` R package (*@Flexible-imputation*). While this allows for realistic imputed values (no imputed value will fall outside the range of observed data), the underlying population distribution of those values could be non-normal. 

Future work could include joining in other academic performance metrics (average SAT/ACT scores, etc.) to see if our proxy variables also have predictive power. The dataset provided is indexed on a high school's *district borough number* (DBN), which is present in several [NYC Open Data](https://opendata.cityofnewyork.us) datasets on education in New York City. As mentioned above, joining in other data sources to augment this data could be a good way to address the omission of any variables that better correlate with college persistence rates.

Overall, identifying the factors that most strongly correlate with academic performance and college persistence can improve in educational policy design. In addition, the public availability of educational data through open data platforms only serves to augment the relationships that help this decision-making.


\newpage

# References

::: {#refs custom-style="Bibliography"}
:::

# Appendices

Below is the code used to generate this report. It's also available on [GitHub here](https://github.com/andrewbowen19/businessAnalyticsDataMiningDATA621/main).
```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```
